% Définit si on compile en recto (pour la lecture sur écran) ou en rectoverso (pour l'impression)
\providecommand{\VarRectoVerso}{oneside}

\input{./preambule}
%\input{./naming}

\title{Optimisation}
\author{Mohamed \textsc{Camar-Eddine}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle

\tableofcontents

\section{Introduction}
%======================%


	\subsection{Aspect \og existence et unicité\fg des solutions}
	%***********************************************************%
	
	On cherche toujours à déterminer un format de problème (P) où il y a une unique solution. Cela rend le problème de l'approximation de la solutoin moins complexe. Un exemple assez simple d'un résultat d'éxistence est le théorème de Weierstrass.
	
	Soit $C$ un compact de $R^n$ et $f: C->R$ continue. Alors $f$ est minoré sur $C$ et il existe un $\bar{x} \in C$ tel que $f(\bar{x}) = \inf_{x \in C} f(x) = \min_{x \in C} f(x)$.
	
	Il peut y avoir plusieurs solutions. Pour s'assurer de l'unicité de $\bar{x}$, on est souvent amené à supposer que $f$ vérifie certaines propriétés particulières telles que la stricte convexité.
	
	\begin{definition}
	Définition de la continuité :
	$f:I->R$, $X_0 \in I$, $f$ est continue en $x_0$ ssi ($ \forall s>0$) $\exists \alpha >0$ tel que $|x-x_0| < \alpha \Rightarrow |f(x)-f(x_0)| < \epsilon$
	\end{definition}
	
	$A$ un sous ensemble de $R$, $m = \inf A$ ssi ($\forall \epsilon>0$) $\exists x_\epsilon \in A$ tel que $m - x_\epsilon < \epsilon$, textit{ie} $m < x_\epsilon + \epsilon$
	
	Conséquence : il existe toujours une suite $(x_k$ dans $A$ telle que $x_k \rightarrow \inf A$.
	
	Définition : $\exists (x_k \in C$ tel que $f(x_k) \rightarrow \inf_{x \in C} f(x)$ ; une telle suite est dite minimisante.
	
	Soit $C$ un compact, soit $(x_k)$ une suite minimisante : $f(x_k) \rightarrow \inf_{x \in C} f(x)$ comme $C$ est compact, on a $(x_k)$ borné $\Rightarrow$ il existe une sous suite $(x_{\phi(k)})$ de $(x_k)$ qui est convergente dans $C$. $x_{\phi(k)} \rightarrow \bar{x} \in C$. $f$ étant continue $f(x_{\phi(k)}) \rightarrow f(\bar{x}) = \inf_{x \in C} f(x)$.
	
	\subsection{Aspect \og conditions nécéssaires d'optimalité\fg}
	%***********************************************************%
	
	Si $\bar{x}$ est un minimum local de $f$ sur $C$, alors on a $P$. (Attention : $P$ est seulement nécessaire)
	
	\subsection{Aspect \og conditions suffisantes d'optimalité\fg}
	%***********************************************************%
	
	
	
	\subsection{Aspect \og algorithmes \fg}
	%**************************************%
	
	$a$ est une valeur d'adhérence d'une suite $(x_n)$ ssi a est la limite d'une sous suite de $(x_n)$.
	
	\subsection{Aspect \og sensibilité aux perturbations, robustesse \fg}
	%**************************************%
	
	$C = {x\in R^n ; h(x) = 0}$
	
	$C_\alpha = {x\in R^n ; h(x) = \alpha}$
	
	
	
	\subsection{Classification des problèmes d'optimisation}
	%******************************************************%
	
	Elle est basée sur les propriétés des données du problème : différentiabilité, convexité, dimension finie ou non, variables continues ou discrètes, etc.
	
	
	\subsection{Exemples}
	%***********************%
	
		\subsubsection{Optimisation sans contraintes}
		%-------------------------------------------%
		
	$f(x, y) = x^3 - y^3 + 3(y-x), (x, y) \in R^2$
	$inf_{(x, y) \in R^2} f(xn y) = -\infty$
	Étude des extremum locaux de $f$ :
	$\nabla f(x, y) = (\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y})^t = (3x^2-3, -3y^2 +3)=-3(x^2-1 , 1-y^2)$
	
	Recherche de points stationnaires (critiques) $\nabla f(x, y) = ( 0, 0)$
	
	$ x^2 = 1 , y^2 = 1$
	
	Les points stationnaires de f sont (-1, -1), (-1, 1), (1, -1) et (1, 1).
	
	%$\nabla^2 f(x, y) = (\cdots) = 3(2x & 0 & 0 & -2y) = 6(x & 0 & 0 & -y) = (r & s & s & t)$
	
	Rappel :
	\begin{itemize}
		\item si $rt-s^2 <0$ alors le point stationnaire en question n'est ni un minimum, ni un maximum local, c'est un point \og col\fg.
		\item si $rt -s^2 >0$ et $r>0$ alors on a un minimum local. 
		\item si si $re -s^2 >0$ et $r<0$ alors on a un maximum local.
	\end{itemize}
	
	Donc (-1, -1) et (1, 1) sont des points cols, (-1, 1) est un maximum local, et (1, -1) est un minimum local.
	
	
		\subsubsection{Optimisation avec contraintes}
		%-------------------------------------------%
		
		Minimiser $f(x, y) = xy$ lorsque $(x, y) \in (E)$ où $(E)$ est l'éllipse d'équation $4x^2+y^2=4$ ($x^2/1^2 + y^2/2^2 = 1$).
		
		\begin{theoreme} (Lagrange)
		
		Si $(x_0, y_0)$ réalise in minimum local sur $E$ et que $\nabla \neq 0$ alors il existe un réel $\lambda$ tel que $\nabla f + \lambda \nabla h = $
		$\lambda$ s'appelle un multiplicateur de Lagrange.
		
		\end{theoreme}
		
		%($E = {(x, y} \in R^2 | h(x, y) = 0}$) \\
		$\nabla f(x, y) = ( x, y)$ \\
		$\nabla h(x, y) = (8x, 2y)$ \\
		$\nabla f + \lambda \nabla h = 0$ \\
		%${ y+8\lambda x=0 (1) && x+2\lambda y=0 (2) && 4x^2+y^2-4=0 (3)$ \\
		$(2) => x = -2\lambda y$ \\
		$(1) => y+8\lambda (-2\lambda y) = 0 => y(1-16 \lambda^2) = 0 => y=0 ou \lambda = +- 1/4$ \\
		mais $y=0 \Rightarrow x=0$ et $(0, 0) not \in (E)$ \\
		donc $\lambda = \frac{1}{4}$ ou $\lambda = -\frac{1}{4}$
		
		\begin{itemize}
		\item Cas $\frac{1}{4}$ : 
		\dots
		$(1/ \sqrt{2}, \sqrt{2})$ et $(-1/ \sqrt{2}, \sqrt{2})$
		\item Cas $-\frac{1}{4}$ : 
		\dots
		$(1/ \sqrt{2}, \sqrt{2})$ et $(-1/ \sqrt{2}, -\sqrt{2})$
		\end{itemize}
		
		$f(1/ \sqrt{2}, \sqrt{2}) = f(-1/ \sqrt{2}, -\sqrt{2}) = \max_{(E)} f$
		$f(-1/ \sqrt{2}, \sqrt{2}) = f(1/ \sqrt{2}, -\sqrt{2}) = \min_{(E)} f$
		
	\section{Rappels et compléments de calcul différentiel}
	%***********************%
	
	$a<b$, soit $f:[a, b]->R$, soit $x_0 \in ]a, b[$.
	$f$ est dérivable en $x_0$ ssi $lim_{h->0, h\neq 0} \frac{ f(x_0+h)-f(x_0) }{ h }$ existe eet vaut $l \in R$.
	De manière équivalente : $f$ est dérivable en $x_0$ ssi il existe une fonction $\epsilon$ définie au voisinage de $0$ vérifiant $\lim_{h->0} \epsilon (h) = 0$ et un réel $l$ tel que $f(x_0+h) = f(x_0)+l\dot h + h \dot \epsilon (h)$.
	Soit $x$ un ouvert de $R^n$ ($n \in ?$), soir $f:\Omega ->R$, soit $x_0 \in \Omega$, f est différentiable en $x_0$ ssi il existe une application linéaire (et donc continue car dim finie) $l:R^n->R$ et une fonction $\epsilon$ définie sur un voisinage de $0$ telle que $f(x_0+h) = f(x_0)+l(h) + h \dot \epsilon (h)$, avec $\lim_{h->0} \epsilon (h) = 0$.
	Comme $l$ est linéaire, $l$ est de la forme $l(h) = \sum _{l=1}^n l_i \dot h_i = <L,h>$ (produit scalaire). 
	%$L= \left(\begin{array}{c} l_1 \\ \vdots \\ l_n \end{array} \right)$ 
	$L= ( l_1 \dots l_n )^t$ 
	est appelé gradient de $f$, on le note $\nabla f$.
	
	$\nabla f(x) = \left( \begin{array}{c} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{array}\right)$
	
	On suppose $f$ différentiable sur $\Omega$, soit $[\bar{x}, \bar{x}+h]$ un segment de $\Omega$.
	\begin{itemize}
		\item Inégalité des accroissements finis : $|f(a+h) -f(a)| \leq \sup_{ x \in ]\bar{x}, \bar{x}+h[ } |\nabla f(x)| \dot ||h||$
		\item Taylor-Lagrange : $f(\bar{x}+h) - f(x) = <\nabla f(\bar{x}+\theta \dot h), h>$, où $0<\theta <1$
		\item Taylor avec reste intégral : $f(\bar{x}+h) = f(\bar{x}) + \int_0^1 <\nabla f(\bar{x}+ t \dot h), h> dt$
	\end{itemize}
	si $f$ est dupposée deux fois différentiable sur $\Omega$ :
	\begin{itemize}
		\item Taylor-Young : $f(\bar{x}+h) = f(\bar{x}) + <\nabla f(\bar{x}, h> + \frac{1}{2} <\nabla^2 f(\bar{x} \dot h), h> + ||h||^2 \epsilon (h)$, avec $\ln \epsilon = 0$
		\item $|f(\bar{x}+h) - f(\bar{x}) - <\nabla f(\bar{x}, h> \leq \frac{1}{2} \sup || \nabla^2 f(\bar{x})|| \dot ||h||^2$, avec $x \in ]\bar{x}, \bar{x}+h[$
		\item Tylor-Lagrange : $f(\bar{x}+h) = f(\bar{x}) + <\nabla f(\bar{x}), h> + \frac{1}{2} <\nabla^2 f(\bar{x}+ \theta \dot h), h>$, où $0< \theta <1$
		\item $f(\bar{x}+h) = f(\bar{x}) + <\nabla f(\bar{x}), h> \int_0^1 (1-t) \dot <\nabla^2 f(\bar{x}+ t \dot h), h> dt$
	\end{itemize}
	
		\subsubsection{rappel sur les fonction convexes}
		%--------------------%
		
		%Soit $C$ un convexe de $R^n$, $f:C->R$, $f$ est dite convexe ssi $\forall (x, x') \in C x C$ et $\forall \alpha \in ]0,1[$, on a $f(\alpha \dot x + (1-\alpha) \dot x') \infeq \alpha \dot f(x) + (1-\alpha) \dot f(x')$.
		
		%$f$ est fortement convexe sur $C$ de module de forte convexité $c>0$ si $f(\alpha \dot x + (1-\alpha) \dot x') \infeq \alpha \dot f(x) + (1-\alpha) \dot f(x') -\frac{1}{2} \dot c \dot \alpha \dot (1-\alpha) \dot ||x'-x||^2$.
		
		\begin{theoreme}
		Soit $f$ différentiable sur un ouvert $\Omega$ de $R^n$ et $C$ un convexe de $\Omega$.
		$f$ est convexe sur $C$ ssi $f(x) >= f(\bar{x}) + <\nabla f(\bar{x}), x-\bar{x}>$, $\forall (x, \bar{x}) \in C x C$.
		\end{theoreme}
		
		$y=f'(\bar{x}) \dot (x-\bar{x}) +f(\bar{x})$
		
		\begin{theoreme}
		Soit $f$ deux fois différentiable, sur un ouvert convexe $C$, alors :
		\begin{itemize}
			\item $f$ est convexe sur $C$ ssi $\nabla^2 f(x)$ est semi-définie positive.
			\item Si $\nabla^2 f(x)$ est semi-définie positive $\forall x \in C$ alors $f$ est strictement convexe sur $C$.
			\item $f$f est fortement convexe sur $C$ de module C ssi la plus petite valeur propore de $\nabla^2 f(x)$ est minorée par $c$, soit : <$\nabla^2 f(x) \dot \lambda, \lambda> >= c \dot --\lambda ||^2$, $\forall x \in C$ et $\forall \lambda \in R^n$.
		\end{itemize}
		\end{theoreme}
		
		
	\subsection{Exemples}
	%***********************%
	
		\subsubsection{Ex 1}
		
		Soit $f:\Omega \R^n$, ($\omega$ ouvert de $R^n$), $x|-> f(x) = A \dot x +b$ où $A \in Mn(R)$ et $b \in R^n$.
		
		Soit $\bar{x}) \in Omega$, $f(\bar{x} +h) = A \dot (\bar{x}+h) + b = A \bar{x} + b + Ah = f(\bar{x}) + Ah + ||h|| \dot 0 = f(\bar{x}) + Ah + ||h|| \dot \epsilon (h)$, où $\epsilon (h) = 0$.
		
		%Donc $f$ est différentiable en $\bar{x}$ et $\nabla f(\bars{x}) = A$.
		
		
		\subsubsection{Ex 2}
		
		Soit $\Phi : \Omega x \Omega \R$ bilinéaire et symétrique, $f: \omega \R$, $x \mapsto f(x) = \Phi(x, x)$.
		Soit $\bar{x} \in \Omega$, $f(\bar{x}+h) = \Phi(\bar{x}+h, \bar{x}+h) = Phi(\bar{x}, \bar{x}) + \Phi(\bar{x}, h) + \Phi(h, \bar{x}) + \Phi(h, h) = f(\bar{x}) + 2 \dot \Phi(\bar{x}, h) + \Phi(h, h)$.
		
		Reste à montrer que $\Phi(h, h) = ||h|| \dot \epsilon (h) \Leftrightarrow \epsilon (h) = \frac{ Phi(h, h) }{ ||h|| }$, avec $\lim{h \rightarrow 0} \epsilon (h) = 0$, \textit{ie} $\Phi(h, h) = o(h)$
		
		Comme $\Phi$ est bilinéaire et continue (car $dim < 2$) on a : $| \Phi (x, x) \leq c \dot ||x||^2$.
		%$| \epsilon (h) | = \frac{  }{  } \leq c \dot \frac{ ||h||^2 }{ ||h|| } = c \dot ||h|| \rightarrow_{h \rightarrow 0} 0$.
		
		%Donc f est différentiable en \bar{x} et $<\nabla f(\bar{x}), h> = 2 \Phi( \bar{x}, h)$.
		
		\subsubsection{Ex 3}
		
		$f(x) = \frac{1}{2} <Ax, x> - <b, x>$ où $b \in R^n$ et $A \in Mn(R)$ symétrique ; étudier la différentiabilité de $f$.
		
		D'après le exemples 1 et 2, $f$ est différentiable et $\nabla f(\bar{x}) = A \bar{x} - b$, (où $<\nabla f(\bar{x}), h> = (A \bar{x} -b, h>$)
		
		Remarque : Si $A$ n'est pas symétrique, alors $\nabla f(\bar{x}) = \frac{1}{2} (A+A^t) \bar{x} - b$.
		
		\subsubsection{Ex 4}
		
		Soit $a \in R^n$ et $V$ un voisinage de $a$, soit $f:V \R$ différentiable en $a$, montrer que $\lim_{t \rightarrow 0} \frac{ f(a+th) - f(a) }{ t } = <\nabla f(a), h>$.
		
		Comme $f$ est différentiable en $a$, on a : $f(a+k) = f(a) + <\nabla f(a), k> + ||k|| \epsilon (k)$, avec $\lim_{k \rightarrow 0} \epsilon (k) = 0$.
		
		En particulier pour $k=th$, on a : $f(a+k) - f(a) = <\nabla f(a), th> + |t| \dot ||k|| \dot \epsilon (th) =  t <\nabla f(a), h> + |t| \dot ||k|| \dot \epsilon (th)$, et $\frac{ f(a+th) - f(a) }{ t } = <\nabla f(a), h> + \frac{|t|}{t} \dot ||k|| \dot \epsilon (th) \Rightarrow \lim_{t \rightarrow 0} \frac{ f(a+th) - f(a) }{ t } = < \nabla f(a), h>$. \P
		
		Si $f$ est telle que $\lim_{t \rightarrow 0} \frac{ f(a+th)-f(a) }{t}$ existe et vaut $l$, on dit que f admet une dérivé directionnelle en $a$ dans la direction $h$. On note cette dérivé $f'(a, h) = l$.
		
		On a vu que si $f$ est différentiable en $a$, alors $f'(a, h)$ existe (attention : la réciproque est fausse).
		
		Exemple : 
		
		$f(x, y) = \frac{x |y|}{\sqrt{x^2+y^2}}$, si $(x, y) \ne (0, 0)$
		
		$f(0, 0) = 0$
		
		$f$ admet des dérivés directionnelles en (0, 0) de direction $(h, k)$ quelconques.
		
		\[ \lim_{t \rightarrow 0} \frac{ f(0, t(h, k)) - f(0) }{t} = \lim \frac{ t(h, k)}{t} = \lim \frac{1}{t} \frac{t |t| h |k|}{ \sqrt{t^2 (h^2+k^2)}} = \frac{h|k|}{\sqrt{h^2+k^2}} \]
		
		Donc $f'(0, (h, k))$ esiste ($h\ne0$ et $k\ne0$), cependant $f$ n'est pas différentiable en $0$.
		
		Si $f$ était différentiable en 0, alors on aurait : \[ f(h, k) = < \nabla f(0), (h, k)> + ||(h, k)|| \epsilon (h, k) \] 
		avec $\lim_{||(h, k)|| \rightarrow 0} \epsilon (h, k) = 0$
		\[ = \frac{ \partial f}{\partial x} (0, 0) h + \frac{ \partial f}{\partial y} (0, 0) k + o(||(h, k)||) \]
		
		Or $ \frac{ \partial f}{\partial x} (0, 0) = f' ((0, 0), e_1) = 0$ où $e_1 = (1, 0)$, et $ \frac{ \partial f}{\partial y} (0, 0) = 0$.
		
		On aurait alors $f(h, k) = o(||(h, k)||) = ||(h, k)|| \epsilon (h, k)$.
		
		Question : A-t-on $\lim_{(h, k) \rightarrow (0, 0)} \epsilon (h, k) = 0 $ ?
		
		$\lim_{(h, k) \rightarrow (0, 0)} \epsilon (h, k) = \lim_{(h, k) \rightarrow (0, 0)} \frac{f(h, k)}{||(h, k)||} = \lim_{(h, k) \rightarrow (0, 0)} \frac{h|k|}{h^2+k^2}$ qui n'existe pas.
		
		En effet, $\lim_{(h, k) \rightarrow (0, 0)} \frac{h|k|}{h^2+k^2} = \lim_{h=k, h \rightarrow 0} \frac{h|h|}{2h^2}$, $h=k \rightarrow 0^+$,  $h=k \rightarrow 0^-$.
		
		\[ \lim_{h \rightarrow 0^+} f(h, h) = \frac{1}{2} \ne -\frac{1}{2} = \lim_{h \rightarrow 0} \frac{h|h|}{2h^2} \]
		
		Donc $f$ n'est pas différentiable en $(0, 0)$.
		
		
		\subsubsection{Ex 5}
		
		Soit $u: \omega \R$ différentiable sur $\Omega$, soit $f: \omega \R$, $ x \mapsto f(x) = (u(x))^2$.
		
		$f$ est elle différentiable sur $\Omega$ ? Su oui, que vaut $ \nabla f(x)$ ?
		
		$a \in \Omega$, $u(a+h) = u(a) + <\nabla u(a), h> + o(h)$
		
		$f(a+h) = (u(a+h))^2 = (u(a) + <\nabla u(a), h> + o(h))^2 = u^2(a) + 2u(a)<\nabla u(a), h> +(<\nabla u(a), h>)^2 + \dots = f(a) + <2u(a)\nabla u(a), h> + o(h)$
		
		$f$ est différentiable et $\nabla f(a) = 2u(a) \nabla u(a)$.
		
		Cauchy-Schwartz : $|<\nabla u(a), h>| \leq ||\nabla u(a)|| \cdot ||h|| \Rightarrow |<\nabla u(a), h>|^2 \leq ||\nabla u(a)||^2 \cdot ||h||^2 \Rightarrow \frac{ (<\nabla u(a), h>)^2 }{||h||} \leq ||\nabla u(a)||^2 \cdot ||h|| \rightarrow_{||h|| \rightarrow 0} 0 \Rightarrow (< \nabla u(a), h>)^2 = o(h)$

		
		
\section{Optimisation linéaire}
%=============================%
		
		
\section{Optimisation sans contraintes}
%=====================================%

	\subsection{1ère partie}
	
		\subsubsection{Condition d'optimalité}
		
		Soit $f : [a, b] \R$ continue et dérivable sur $[a,  b]$.
		Soit $\bar{x} \in [a, b]$ tel que $\bar{x}$ réalise un minimum local de $f$.
		
		Théorème (condition d'optimalité du 1er ordre)
		
		Avec les hypothèses vues plus haut, on a $f'(x) = 0$.
		
		Preuve (faite)
		
		Attention : $f'(x) = 0$ n'est qu'une condition nécessaire, elle n'est pas suffisante ! (ex : $f(x) = x^3$)
		
		Théorème (Weierstrass) :
		
		Si $f : [a, b]$ est continue, alors $f$ atteint ses bornes dans $[a, b]$, \textit{ie} il existe $x_1$ et $x_2$ $ \in [a, b]$ tels que $\displaystyle f(x_1) = \inf_{x \in [a, b]} f(x) = \min_{[a, b]} f$ et $\displaystyle f(x_2) = \sup_{x \in [a, b]} f(x) = \max_{[a, b]} f$ \\
		
		Preuve (déjà faite).\\
		
		Définition :
		
		Soit $f : R \R$, on dit que $f$ est c\oe rcive si \[ \lim_{|x| \rightarrow \infty} f(x) = + \infty \]
		
		(ex : $|f(x)| \geq c|x|^\alpha$, avec $\alpha > 0$)
		
		Soit $f : R \R$ continue et c\oe rcive, alors $f$ atteint un minimum global.\\
		
		Rappel : $\displaystyle \lim_{x \rightarrow x_0} = l$ ssi ($\forall \epsilon >0$) $\exists \alpha >0$ tel que $|x-x_0| \Rightarrow |f(x) - l| < \epsilon$\\
		
		Preuve :
		
		Comme $\displaystyle \lim_{|x| \rightarrow + \infty} = +\infty$, on a pour tout $M>0$ il existe $\alpha >0$ tel que $|x|>\alpha \Rightarrow f(x)>M$
		Doc $f$ est minorée par $M$ sur $]-\infty, \alpha[ \cup ]-\alpha, +\infty[$. Par ailleurs, $f$ est continue sur le fermé borné $]-\alpha, \alpha[$.
		Cela implique (Weiestrass) que $f$ est bornée sur $]-\alpha, \alpha[$ et $y$ atteint ses bornes.
		Il existe $ \bar{x} \in ]-\alpha, \alpha[$ tel que $\displaystyle f(\bar{x}) = \inf_{\bar{x} \in ]-\alpha, \alpha[} f(x)$
		
		Si $\displaystyle \inf_{\bar{x} \in ]-\alpha, \alpha[} f(x) \leq M$ alors $\displaystyle f(x) > \inf_{x \in R} f(x) = \inf_{\bar{x} \in ]-\alpha, \alpha[} f(x)$
		
		Si $\displaystyle \inf_{\bar{x} \in ]-\alpha, \alpha[} f(x) > M$ alors sur $]-\alpha, \alpha[$ on a $\displaystyle f(x) \geq \inf_{\bar{x} \in ]-\alpha, \alpha[} f(x) > M$
		
		(Choisir $M = f(x_0)$ avec $x_0 \in R$, on a : $\inf_{]-\alpha, \alpha[} f(x) > f(x_0) > f(x)$, $\forall |x| > \alpha$)
		
		\subsubsection{Conditions d'optimalité du second ordre}
		
		Soit $f : [a, b] \R$ continue sur $[a, b]$ et dérivable sur $]a, b[$.
		
		Théorème (conditions nécessaire) : 
		
		Soit $\bar{x} \in ]a, b[$ tel que $\bar{x}$ réalise un minimum local de $f$. ALors $f'(\bar{x}) = 0$ et $f''(\bar{x}) \geq 0$.
		
		En effet, $f$ admet un minimum local en $\bar{x} \Rightarrow \exists \alpha >0$ tel que $|x-\bar{x}|<\alpha \Rightarrow f(x) \geq f(\bar{x})$.
		
		D'après Taylor-Young, on a : $\displaystyle f(\bar{x}+h) -f(\bar{x}) = \frac{1}{2} h^2 f''(\bar{x}) + o(h^2)$, $\forall |h|<\alpha$
		
		On divise par $h^2 \ne 0$, et on obtient : $\displaystyle 0 \leq \frac{f(\bar{x}+h) - f(\bar{x}) }{ h^2 } = \frac{1}{2} f''(\bar{x}) + o(1) $, et $o(1) \sim \epsilon(h)$ tel que $\lim_{h \rightarrow 0} \epsilon(h) = 0$.
		Prendre la limite quand $h\rightarrow 0$ : $\frac{1}{2} f''(\bar{x}) \geq 0$.
		
		
		Théorème (condition suffisante) :
		
		$f : [a, b] \R$ continue et deux fois dérivable sur $]a, b[$. Soit $\bar{x} \in ]a, b[$ tel que $f'(\bar{x}) = 0$ et $f''(\bar{x}) \geq 0$. ALors $\bar{x}$ réalise un minimum local strict de $f$ sur $[a, b]$.
		
		En effet,par Taylor-Young, on a: $f(\bar{x}+h) -f(\bar{x}) = \frac{1}{2} h^2 f''(\bar{x}) + h^2 \epsilon(h)$ et $signe( f(\bar{x}+h) - f(\bar{x}) ) = signe( \frac{1}{2} f''(\bar{x}) + \epsilon(h))$.
		
		Or $\displaystyle \epsilon(h) \longrightarrow_{h\rightarrow 0} 0$, donc pour $h$ suffisamment petit, on a : 
		\[ \frac{1}{2} f''(\bar{x}) + \epsilon(h) >0\]
		\[ \epsilon(h) \longrightarrow_{h\rightarrow 0} 0 \Rightarrow |\epsilon(h)| < \frac{1}{4} f''(\bar{x}) \textit{ pour h petit}\]
		\[-\frac{1}{4} f''(\bar{x}) < \epsilon(h) < \frac{1}{4} f''(\bar{x})\]
		\[0< \frac{1}{4} f''(\bar{x}) < \frac{1}{4} f''(\bar{x}) + \epsilon(h) \]
		Or $f(\bar{x}+h) - f(\bar{x}) >0$ pour $h$ suffisamment petit, $ \Rightarrow \bar{x}$ réalise un minimum local strict.
		
		\subsection{2ème partie : fonction de plusieurs variables}
		
		Théorème :
		
		Soit $f : B(\bar{x}, \alpha) \rightarrow \R$,  (où $B(\bar{x}, \alpha) \subset \R^n$ est la boule de rayon $\alpha$ centrée sur $\bar{x}$) continue et différentiable sur $B(\bar{x}, \alpha)$.
		
		On suppose que $f$ admet un minimum local en $\bar{x}$, alors $\nabla f(\bar{x}) = 0$ (condition d'optimalité du premier ordre.
		
		Preuve : (exercice)
		
		Par Taylor-Young on a : $f(\bar{x}+h)-f(\bar{x}) = < \nabla f(\bar{x}) ,h> + ||h|| \epsilon(h)$ avec $\lim_0 \epsilon = 0$.
		
		%TODO
		
		Le théorème de Weierstrass reste vraie dans $\R^n$.
		$f:X\rightarrow \R$ où $X$ est un compact de $R^n$.
		Si $f$ est continue sur $X$, alors $f$ y atteint ses bornes.
		
		Théorème : 
		
		Soit $f:R^n \R$ continue et c\oe rcive, alors $f$ admet un minimum global.
		
		Théorème : 
		
		(Condition Nécessaire)
		
		Soit $f : B(\bar{x}, \alpha) \subset \R^n \R$ continue et deux fois différentiable.
		
		Si $\bar{x}$ réalise un minimum local, alors $\nabla f(\bar{x}) = 0$ et $\nabla^2 f(\bar{x})$ est semi-définie positive.
		
		(Condition Suffisante)
		
		Soit $\bar{x}$ tel que $\nabla f(\bar{x}) = 0$ et $\nabla^2 f(\bar{x})$ soit définie positive, alors $\bar{x}$ réalise un minimum local de $f$.
		
		\begin{exemple}
		
		On veut minimiser la fonction quadratique $f(x) = <Ax, x> +2<b, x> +c$ où $A$ est symétrique définie positive $\forall x \in \R^n$.
		($\nabla f(\bar{x}) = 2A\bar{x} +2b$)
		
		$f$ est un polynome de degré 2 en $x_i$ $\Rightarrow f$ est continue sur $\R^n$.
		
		$f$ est c\oe rcive et donc 
	 	$ f(x) = <Ax, x>  +2<b, x> +c \geq \lambda ||x||^2 + 2<b, x> +c \geq \lambda ||x||^2 - 2||b|| \cdot ||x|| +c $
		
		(car par Cauchy-Schwartz : $|<b, x>| \leq ||b|| \cdot ||x||$)
		
		où $\lambda$ est la plus petite des valeurs propres de $A$, et comme $A$ est symétrique définie positive, on a $\lambda >0$.
		
		Or $\displaystyle \lim_{||x|| \rightarrow +\infty}( \lambda ||x||^2 -2||b|| \cdot ||x|| +c) = +\infty$.
		$\displaystyle \Rightarrow \lim_{||x|| \rightarrow +\infty}f(x) = +\infty \Rightarrow f$ est c\oe rcive.
		
		Si $f$ est continue et c\oe rcive, alors $f$ admet un minimum global atteint en au moins un $\bar{x}$ solution de $\nabla f(\bar{x}) = 0$.
		
		$\nabla f(\bar{x}) = 2A\bar{x} +2b = 0 \Rightarrow \bar{x} = A^{-1} b$, et comme $A$ est symétrique définie positive, $\Rightarrow A$ est inversible.
		
		$f(x) = <Ax, x> +2<b, x> +c$ est strictement convexe $\Rightarrow$ il y a unicité de la solution.
		
		\end{exemple}
		
		\begin{exemple}%[droite de régression]
		
		Soient $N$ points $\{(x_i, y_i), 1\leq i \leq N\}$ d'abscisses strictement croissantes.
		
		Déterminer une droite d'équation $y=ax+b$ telle que la quantité $\displaystyle f(a, b) = \sum_{i=1}^N (y_i-(ax_i+b))^2$ soit minimale.
		
		$f$ est continue et différentiable en tout $(a, b) \in \R^2$.
		On veut que $\nabla f(a, b) = 0$.
		
		Notations :
		$\displaystyle \bar{x} = \frac{1}{N} \sum_{i=1}^N x_i$, 
		$\displaystyle \bar{y} = \frac{1}{N} \sum_{i=1}^N y_i$, 
		$\displaystyle s_2 = \frac{1}{N} \sum_{i=1}^N x_i^2$, 
		$\displaystyle s_1 = \frac{1}{N} \sum_{i=1}^N x_i y_i$.
		
		\begin{eqnarray*}%[\{]
 \frac{\partial f}{\partial a} = \sum_{i=1}^N 2(y_i-(ax_i+b)) \\ \frac{\partial f}{\partial b} = \sum_{i=1}^N 2(y_i-(ax_i+b)) \end{eqnarray*}

		\end{exemple}
		
		
\section{Optimisation avec contraintes \\ Conditions de Lagrange \\ Conditions de Karush-Kuhn-Tucker }
%=====================================%

\subsection{Optimisation avec contraintes de type égalité - Condition de Lagrange}
Soient $f : \R^n \rightarrow \R$, non nécessairement linéaire et K un sous ensemble de $\R^n$.\\
Nous nous intéressons au problème d'optimisation suivant :
 \[ (\mathcal{P})\begin{cases}\min f(x) \\ x \in K \end{cases} \]

%TODO

Nous nous intéressons au problème d'optimisation suivant : \[ (P) : \min f(x), x \in K \]

Il s'agit de trouver $\bar{x} \in K$ tel que \[ f(\bar{x}) = \inf f(x) \]

% TODO

Généralement $K$ est définie par des fonctions $h_i$, $1 \leq i \leq p$, de la manière suivante : \[ TODO \]

On parle de contraintes de type égualité.

%TODO

\begin{exemple}
	Considérons le problème de minimisation, avec une seule contrainte, suivant : \[ TODO \]	
	
	L'ensemble des contraintes est le cercle de centre 0 et de rayon $\sqrt 2$ \[ K = C(0, \sqrt(2)) \].
	
	On considère les droites d'équation \[  \]
	
	Les droites $(\Delta_a)$ se déplacent vers le \og sud-ouest \fg. $f$ atteint donc son minimum en \[ \bar{x} = (-1, -1) \]
	
	En $\bar{x}$, on a $\displaystyle \nabla f = ...$ et $ $ où $h(x) = x_1^2 + x_2^2 - 2$
	
	On remarque que $ \nabla h( \bar{x} ) = -2  \nabla f( \bar{x} ) $
	
	
\end{exemple}

%%%%%%%%%%%%%%%%%%%%%%% TODO %%%%%%%%%%%%%%%%%%%%%%

\begin{remarque} (sur la droite de régression)

	Quand on veut approcher un nuage de points $(x_i, y_i)_{1 \leq i \leq n}$ par une droite $y = ax +b$, la mesure naturelle serait la somme des valeurs absolues des résidus $r_i = y_i -(ax_i+b)$ mais il est plus commode de considérer la somme des $r_i^2$.

	\[ 1 \leq \frac{ |r_1| + \dots + |r_n| }{ (r_i^2 + \dots + r_n^2)^{\frac{1}{2}} } \leq \sqrt{n} \]
	
	Les deux erreurs ne sont proches que si n est petit.
\end{remarque}

\begin{exemple}
	Soient $1 < p \leq q$,
	\begin{align*}
	f : \R^n & \rightarrow  \R \\
	x & \mapsto \sum_{i=1}^n (x_i)^p
	\end{align*}
	\begin{align*}
	g : \R^n & \rightarrow  \R \\
	x & \mapsto \sum_{i=1}^n (x_i)^q
	\end{align*}
	\[ S = \{ x \in \R, \quad g(x) = 1 \} \]
	
	Étudier le problème suivant :
	\[ \mathcal{P} \begin{cases} f(x), \\ x \in S \end{cases} \]
	
	
\end{exemple}

\subsection{Condition de type inégalité (condition de Karush Kuhn Tucker)}

\begin{exemple}
	Reprenons le problème $(P1)$ avec une contrainte d'inégalité.
	\[ (\mathcal{P}) : \min f(x) = x_1 + x_2 \]
	sachant que $g(x) = x_1+x_2 -2 \leq 0$.
	
	$S = \{ g(x) = 0 \}$ est le cercle $C(0, \sqrt{0})$ qui est compact. $f$ étant continue, $f$ atteint son minimum en un point $\bar{x} \in S$
	
	$\bar{x} \in S$(avec un rond : intérieur de S) sinon $\nabla f$ serait nul, ce qui n'est pas le cas.
	
	La condition de Lagrange dit : $\exists \lambda \in \R$ tel que : \[ \nabla f + \lambda \cdot \nabla h = 0 \]
	$\Rightarrow \bar{x} = (-1, -1) et \lambda = \frac{1}{2}$ ou $\bar{x} = (1, 1) et \lambda = -\frac{1}{2}$
	
\end{exemple}
	
	\subsubsection{CN d'optimalité : règle des multiplicateurs de KKT}
	
	On considère le problème suivant : \[ \begin{cases}\min f(x) \\ g_j(x) \leq 0 , \quad 1 \leq j \leq p \end{cases} \]
	
	On suppose que :
\begin{itemize}
	\item $\bar{x}$ est un point admissible
	\item $f:B(\bar{x},\alpha)$ est différentiable
	\item toutes les fonctions $(g_j)_j$ sont continuement différentiables en $\bar{x}$
\end{itemize}

On désigne par $J(x)$ l'ensembles des indices des contraintes actives (saturées) au point $\bar{x}$, ie $J(x) = \{ j \in \{ 1, \dots, p \}, \quad g_j(\bar{x}) = 0 \} $.

On ajoute l'hypothèse suivante, dite de qualification des contraintes : les gradients des contraintes actives au point $\bar{x}$ : $\nabla g_j(\bar{x}), {j\in J(\bar{x})}$ sont linéairement indépendants.

\begin{theoreme} (CN de KKT)
	
	Si $\bar{x}$ réalise un minimum local de $f$, alors il existe un vecteur $\mu \in \R_+^p$, $\mu \ne 0$ de multiplicateur positif tel que \[ \begin{cases} \displaystyle \nabla f(\bar{x}) + \sum_{j=1}^p \mu _j \nabla g_j (\bar{x}) = 0 \quad (i) \\ \mu _j g_j(\bar{x}) = 0, \quad \forall i \leq j \leq p \quad (ii) \end{cases} \]
	
\end{theoreme}

\begin{remarque}
La condition $(ii)$ signifie que si $j \not \in J(\bar{x})$ alors $\mu_j = 0$.

La condition de KKT devient \[ \nabla f(\bar{x}) + \sum_{j \in J(x)} \mu _j \nabla g_j (\bar{x}) = 0 \]
\end{remarque}

\begin{theoreme} (CS d'optimalité de KKT pour les problèmes convexes)

Supposons $f$ et $g_j$ convexes définies sur un convexe $C$ de $\R^n$. Si les conditions de KKt sont satisfaites en $\bar{x}$, alors $\bar{x}$ réalise un minimum global.
\end{theoreme}

\subsection{Optimisation avec contraintes mixtes}

Considérons le problème d'optimisation suivant avec les deux types de contraintes : \[ (\mathcal{P}) \begin{cases}\min f(x) \\ h_i(x) = 0 , \quad 1 \leq j \leq n \\ g_j(x) \leq 0 , \quad i \leq j \leq p \end{cases} \]

On désigne par $J(\bar{x})=\{j \in \{1, \dots , p\}; g_j(\bar{x}) = 0\}$.
On ajoute les hypothèses suivantes de qualification de contraintes :
\begin{itemize}
	\item[1)] $\exists \lambda_0 \in \R^n$ tel que :
	\begin{itemize}
		\item $<\nabla h_i(\bar{x}), \lambda_0> \; = 0, \quad \forall i \in \{ 1, \dots , n \}$
		\item $<\nabla g_j(\bar{x}), \lambda_0> \; < 0, \quad \forall j \in \{ 1, \dots , p \}$
	\end{itemize}
	\item[2)] $\nabla h_1(\bar{x}), \dots, \nabla h_n(\bar{x})$ sont linéairement indépendants
\end{itemize}

\begin{theoreme} (CN d'optimalité de KKT)

Si $\bar{x}$ réalise un minimum local de $f$, alors $\exists \lambda \in \R^n \text{  et } \mu \in \R^p$ tel que :
	\begin{itemize}
		\item[(i)] $\displaystyle \nabla f(\bar{x}) + \sum_{i=1}^n \lambda _i \nabla h_j (\bar{x}) + \sum_{j=1}^p \mu _j \nabla g_j (\bar{x}) = 0$
		\item[(ii)] $ \mu _j \geq 0 \quad \forall j \in \{ i, \dots , p \}$
		\item[(iii)] $ \mu _j \cdot g_j(\bar{x}) = 0 \quad \forall j \in \{ 1, \dots , p \}$
	\end{itemize}
\end{theoreme}

\begin{theoreme} (CS d'optimalité dans le cas convexe)

Supposons que $f$ et $g_j$ soient convexes et que les $h_i$ soient affines. ALors si les conditions de KKT sont satisfaites au point $\bar{x}$, alors ce point est un minimum de $f$.
\end{theoreme}
\begin{exemple} %1

	Soient $ \displaystyle f(x) = \sum_{i=1}^n |x_i|^p $, 
	$ \displaystyle g(x) = \sum_{i=1}^n |x_i|^q $, et
	$ \displaystyle C = \{ x \in \R ; g(x) = 1 \} $
	
	%\[ \min TODO \]
	
	Comme $p>1$ et $q>1$, les fonctions $f$ et $g$ sont différentiables sur $\R^n$, et on obtient :\\ $\displaystyle (\nabla f)_i = \begin{cases} p |x_i|^{p-1} \quad \text{ si }x_i\geq 0 \\  -p |x_i|^{p-1} \quad \text{ si } x_i < 0  \end{cases}$ 
	
	La condition de Lagrange s'écrit : $\exists \lambda \in \R$ tel que 
	\[ \nabla f(\bar{x}) + \lambda \nabla g(\bar{x}) = 0 \]
	\[ (\nabla f(\bar{x}))_i + (\lambda \nabla g(\bar{x}))_i = 0 , \quad \forall i \in \{ 1, \dots, n \} \]
	\[ p|x_i|^{p-1} + \lambda q|x_i|^{q-1} = 0 , \quad \forall i \in \{ 1, \dots, n \} \]
	%TODO	La méthode de ré-injection tourne en rond.
	$ \Rightarrow \lambda = - \frac{p}{q}|x_i|^{p-q} $ existe et est unique (au point $\bar{x}$). On en déduit que les valeurs absolues de tous les $x_i$ sont égales.
	Notons $k$ le nombre des $x_i$ non nuls ($k \geq 1$ car sinon $g(x) = 0$ et $\bar{x} \not \in C$)
	$f(\bar{x}) = k \alpha ^p$ où $\alpha = |x_i|$.
	
	On veut maximiser $f$, on sait que $\bar{x} \in C \Rightarrow g(\bar{x}) = 1 \Rightarrow \sum |x_i|^q = 1 \Rightarrow k |x_i|^q = 1 \Rightarrow k \alpha^q = 1 \Rightarrow \alpha = k^{-\frac{1}{q}} \Rightarrow f(\bar{x}) = k \alpha^p = k \cdot k^{-\frac{p}{q}} = k^{1-\frac{p}{q}} $
	
	Or $1-\frac{p}{q} \geq 0$, donc $f(\bar{x})$ est maximal pour $k=n$, $f(\bar{x}) = n^{1-\frac{p}{q}}$ et $\bar{x} = (^+_- n^{-\frac{1}{q}}, \dots, ^+_- n^{-\frac{1}{q}})$.
	
\end{exemple}

\begin{exemple}


 	% 2
	Soit $A$ un matrice de $Mn(\R)$. Notons $\lambda _1$ sa plus petite valeur propre. On suppose que $\lambda _0 \leq 0$. On considère le problème d'optimisation suivant : \[ \begin{cases}\min f(x) = <Ax, x> \quad 0 \not \in sp(A)\\ x \in C = \{ x \in \R, ||x||^2 \leq 1 \} \end{cases} \]
	
	$C$ est fermé borné donc $C$ est compact ;
	$f$ est continue sur $\R^n$, donc $f$ atteint son minimum en un point $\x$ de $C$.
	
	% 2 %
	Posons $ g(x) = ||x||^2 - 1 $, $g$ est différentiable sur $\R^n$ tout entier.\\
Si $\bar{x}$ réalise le minimum de $f$ alors $\bar{x}$ est sur le bord de $C$. Prouvons le par l'absurde.\\
On suppose que $\bar{x} \in \textit{ l'intérieur de }C $.
Cela implique que : \[ \exists \alpha>0 \text{ tel que la boule } B(\bar{x},\alpha)  \]
\[ f : B(x,\alpha) \rightarrow R \]
$f$ est différentiable sur un voisinage de $\bar{x}$ et atteint son minimum en $\bar{x}$.
	
	$\Ra \nabla f(\x) = 0$
	Or $\nabla f(\x) = \not 2A\x$
	on aurait $2A\x = 0 \Lra A\x = 0$ qui est interdit car $0 \not \in sp(A)$
	Donc $\x \in \partial C \text{(bord de C)} \Ra g(\x) = 0 \Ra$ la contrainte est active en $\x$.
	
	% 4 %
	\[ \exists \lambda > 0 \text{ tel que } \nabla f(\bar{x}) + \lambda.\nabla g(\bar{x}) = 0 \]
\[ \Rightarrow 2A\bar{x} + 2\lambda\bar{x} = 0 \]
\[ \Rightarrow A\bar{x} = - \lambda\bar{x}\]

Cela implique que $\bar{x}$ est un vecteur propre associé à la valeur propre $-\lambda$. $\bar{x}$ est à chercher dans l'ensemble des vecteurs propres unitaires associés à des valeurs propres négatives.
	
	$f(\x) = <A\x, \x> = <-\lambda \x, \x> = -\lambda <\x, \x> = - \lambda$ \\
	Donc pour minimiser $f$f, on prend $\x$, un vecteur propre unitaire associé à $\lambda_1$.\\
	On aura au préalable montré que la contrainte est qualifiée en $\x$ : on a $\nabla g(\x) = 2\x \ne 0$ (car $||\x||=1$).
	
\end{exemple}

\section{Algorithmes de minimisation sans contraintes}
\subsection{Résolution approchée d'équations non linéaires}
\subsection{Méthode de Newton}



\subsection{Méthode de la sécante}
\subsection{Méthodes de descente}
\subsection{Algorithme du gradient à pas fixe}
\subsection{Algorithme du gradient à pas optimal}


% 28/03/2011





	
%\sectionEt{Conclusion}
%====================%



%Pour approfondir voir les références \cite{minoux} \cite{bierlaine} \cite{luenberger}


\bibliographystyle{./plain-fr}
\bibliography{biblio}


\end{document}
