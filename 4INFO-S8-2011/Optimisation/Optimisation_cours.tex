% Définit si on compile en recto (pour la lecture sur écran) ou en rectoverso (pour l'impression)
\providecommand{\VarRectoVerso}{oneside}

\input{./preambule}
%\input{./naming}

\title{Optimisation}
\author{Mohamed \textsc{Camar-Eddine}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath}
\begin{document}

\maketitle

\tableofcontents

\newpage{}

\section{Introduction}
%======================%


	\subsection{Aspect \og existence et unicité\fg des solutions}
	%***********************************************************%
	
	On cherche toujours à déterminer un format de problème $(\mathcal{P})$ où il y a une unique solution. Cela rend le problème de l'approximation de la solution moins complexe. Un exemple assez simple d'un résultat d'existence est le théorème de Weierstrass.
	
	Soit $C$ un compact de $R^n$ et $f: C->R$ continue. Alors $f$ est minoré sur $C$ et il existe un $\bar{x} \in C$ tel que $f(\bar{x}) = \inf_{x \in C} f(x) = \min_{x \in C} f(x)$.
	
	Il peut y avoir plusieurs solutions. Pour s'assurer de l'unicité de $\bar{x}$, on est souvent amené à supposer que $f$ vérifie certaines propriétés particulières telles que la stricte convexité.
	
	\begin{definition}
	Définition de la continuité :
	$f:I->R$, $X_0 \in I$, $f$ est continue en $x_0$ ssi ($ \forall s>0$) $\exists \alpha >0$ tel que $|x-x_0| < \alpha \Rightarrow |f(x)-f(x_0)| < \epsilon$
	\end{definition}
	
	$A$ un sous ensemble de $R$, $m = \inf A$ ssi ($\forall \epsilon>0$) $\exists x_\epsilon \in A$ tel que $m - x_\epsilon < \epsilon$, textit{ie} $m < x_\epsilon + \epsilon$
	
	Conséquence : il existe toujours une suite $(x_k$ dans $A$ telle que $x_k \rightarrow \inf A$.
	
	\begin{definition} $\exists (x_k \in C$ tel que $f(x_k) \rightarrow \inf_{x \in C} f(x)$ ; une telle suite est dite minimisante.
	\end{definition}
	
	Soit $C$ un compact, soit $(x_k)$ une suite minimisante : $f(x_k) \rightarrow \inf_{x \in C} f(x)$ comme $C$ est compact, on a $(x_k)$ borné $\Rightarrow$ il existe une sous suite $(x_{\phi(k)})$ de $(x_k)$ qui est convergente dans $C$. $x_{\phi(k)} \rightarrow \bar{x} \in C$. $f$ étant continue $f(x_{\phi(k)}) \rightarrow f(\bar{x}) = \inf_{x \in C} f(x)$.
	
	\subsection{Aspect \og conditions nécéssaires d'optimalité\fg}
	%***********************************************************%
	
	Si $\bar{x}$ est un minimum local de $f$ sur $C$, alors on a $P$. (Attention : $P$ est seulement nécessaire)
	
	\subsection{Aspect \og conditions suffisantes d'optimalité\fg}
	%***********************************************************%
	
	\subsection{Aspect \og algorithmes \fg}
	%**************************************%
	
	$a$ est une valeur d'adhérence d'une suite $(x_n)$ ssi a est la limite d'une sous suite de $(x_n)$.
	
	\subsection{Aspect \og sensibilité aux perturbations, robustesse \fg}
	%**************************************%
	
	$C = {x\in R^n ; h(x) = 0}$
	
	$C_\alpha = {x\in R^n ; h(x) = \alpha}$
	
	\subsection{Classification des problèmes d'optimisation}
	%******************************************************%
	
	Elle est basée sur les propriétés des données du problème : différentiabilité, convexité, dimension finie ou non, variables continues ou discrètes, etc.
	
	
	\subsection{Exemples}
	%***********************%
	
		\subsubsection{Optimisation sans contraintes}
		%-------------------------------------------%
		
	$f(x, y) = x^3 - y^3 + 3(y-x), (x, y) \in R^2$
	$inf_{(x, y) \in R^2} f(xn y) = -\infty$
	Étude des extremum locaux de $f$ :
	$\nabla f(x, y) = (\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y})^t = (3x^2-3, -3y^2 +3)=-3(x^2-1 , 1-y^2)$
	
	Recherche de points stationnaires (critiques) $\nabla f(x, y) = ( 0, 0)$
	
	$ x^2 = 1 , y^2 = 1$
	
	Les points stationnaires de f sont (-1, -1), (-1, 1), (1, -1) et (1, 1).
	
	$\nabla^2 f(x, y) = \begin{pmatrix} \frac{\partial^2 f}{\partial x^2} &\frac{\partial^2 f}{\partial x \partial y} \\ \frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2} \end{pmatrix} 
	= \begin{pmatrix} 32x & 0 \\ 0 & -2y \end{pmatrix}
	= 6\begin{pmatrix} x & 0 \\ 0 & -y\end{pmatrix} 
	= \begin{pmatrix} r & s \\ s & t\end{pmatrix}$

	
	\textbf{Rappel} :
	\begin{itemize}
		\item si $rt-s^2 <0$ alors le point stationnaire en question n'est ni un minimum, ni un maximum local, c'est un point \og col\fg.
		\item si $rt -s^2 >0$ et $r>0$ alors on a un minimum local. 
		\item si si $re -s^2 >0$ et $r<0$ alors on a un maximum local.
	\end{itemize}
	
	Donc (-1, -1) et (1, 1) sont des points cols, (-1, 1) est un maximum local, et (1, -1) est un minimum local.
	
	
		\subsubsection{Optimisation avec contraintes}
		%-------------------------------------------%
		
		Minimiser $f(x, y) = xy$ lorsque $(x, y) \in (E)$ où $(E)$ est l'éllipse d'équation $4x^2+y^2=4$ ($x^2/1^2 + y^2/2^2 = 1$).
		
		\begin{theoreme} (Lagrange)
		Si $(x_0, y_0)$ réalise in minimum local sur $E$ et que $\nabla \neq 0$ alors il existe un réel $\lambda$ tel que $\nabla f + \lambda \nabla h = $
		$\lambda$ s'appelle un multiplicateur de Lagrange.
		\end{theoreme}
		
		($E = {(x, y) \in \R^2 | h(x, y) = 0}$)\\
		\[\begin{cases} \nabla f(x, y) = ( x, y) \\ \nabla h(x, y) = (8x, 2y)\\ \nabla f + \lambda \nabla h = 0 \end{cases} \] \\
		\[\begin{cases} y+8\lambda x=0 \quad(1) \\ x+2\lambda y=0 \quad(2) \\ 4x^2+y^2-4=0 \quad(3)\end{cases}\] \\
		$(2) => x = -2\lambda y$ \\
		$(1) => y+8\lambda (-2\lambda y) = 0 => y(1-16 \lambda^2) = 0 => y=0 ou \lambda = +- 1/4$ \\
		mais $y=0 \Rightarrow x=0$ et $(0, 0) not \in (E)$ \\
		donc $\lambda = \frac{1}{4}$ ou $\lambda = -\frac{1}{4}$
		
		\begin{itemize}
		\item Cas $\frac{1}{4}$ : 
		\dots
		$(1/ \sqrt{2}, \sqrt{2})$ et $(-1/ \sqrt{2}, \sqrt{2})$
		\item Cas $-\frac{1}{4}$ : 
		\dots
		$(1/ \sqrt{2}, \sqrt{2})$ et $(-1/ \sqrt{2}, -\sqrt{2})$
		\end{itemize}
		
		$f(1/ \sqrt{2}, \sqrt{2}) = f(-1/ \sqrt{2}, -\sqrt{2}) = \max_{(E)} f$
		$f(-1/ \sqrt{2}, \sqrt{2}) = f(1/ \sqrt{2}, -\sqrt{2}) = \min_{(E)} f$
		
	\section{Rappels et compléments de calcul différentiel}
	%***********************%
	
	$a<b$, soit $f:[a, b]->R$, soit $x_0 \in ]a, b[$.
	$f$ est dérivable en $x_0$ ssi $lim_{h->0, h\neq 0} \frac{ f(x_0+h)-f(x_0) }{ h }$ existe eet vaut $l \in R$.
	De manière équivalente : $f$ est dérivable en $x_0$ ssi il existe une fonction $\epsilon$ définie au voisinage de $0$ vérifiant $\lim_{h->0} \epsilon (h) = 0$ et un réel $l$ tel que $f(x_0+h) = f(x_0)+l\dot h + h \dot \epsilon (h)$.
	Soit $x$ un ouvert de $R^n$ ($n \in ?$), soir $f:\Omega ->R$, soit $x_0 \in \Omega$, f est différentiable en $x_0$ ssi il existe une application linéaire (et donc continue car dim finie) $l:R^n->R$ et une fonction $\epsilon$ définie sur un voisinage de $0$ telle que $f(x_0+h) = f(x_0)+l(h) + h \dot \epsilon (h)$, avec $\lim_{h->0} \epsilon (h) = 0$.
	Comme $l$ est linéaire, $l$ est de la forme $l(h) = \sum _{l=1}^n l_i \dot h_i = <L,h>$ (produit scalaire). 
	%$L= \begin{pmatrix} l_1 \\ \vdots \\ l_n \end{pmatrix}$ 
	$L= ( l_1 \dots l_n )^t$ 
	est appelé gradient de $f$, on le note $\nabla f$.
	
	\[\nabla f(x) = \left( \begin{array}{c} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{array}\right)\]
	
	On suppose $f$ différentiable sur $\Omega$, soit $[\bar{x}, \bar{x}+h]$ un segment de $\Omega$.
	\begin{itemize}
		\item Inégalité des accroissements finis : \[|f(a+h) -f(a)| \leq \sup_{ x \in ]\bar{x}, \bar{x}+h[ } |\nabla f(x)| \dot ||h||\]
		\item Taylor-Lagrange : \[f(\bar{x}+h) - f(x) = <\nabla f(\bar{x}+\theta \dot h), h>, \quad \textit{ où } 0<\theta <1\]
		\item Taylor avec reste intégral : \[f(\bar{x}+h) = f(\bar{x}) + \int_0^1 <\nabla f(\bar{x}+ t \dot h), h> dt\]
	\end{itemize}
	si $f$ est supposée deux fois différentiable sur $\Omega$ :
	\begin{itemize}
		\item Taylor-Young : \[f(\bar{x}+h) = f(\bar{x}) + <\nabla f(\bar{x}, h> + \frac{1}{2} <\nabla^2 f(\bar{x} \dot h), h> + ||h||^2 \epsilon (h), \quad \textit{ avec } \ln \epsilon = 0\]
		\item \[|f(\bar{x}+h) - f(\bar{x}) - <\nabla f(\bar{x}, h> \leq \frac{1}{2} \sup || \nabla^2 f(\bar{x})|| \dot ||h||^2, \quad \textit{ avec } x \in ]\bar{x}, \bar{x}+h[\]
		\item Tylor-Lagrange : \[f(\bar{x}+h) = f(\bar{x}) + <\nabla f(\bar{x}), h> + \frac{1}{2} <\nabla^2 f(\bar{x}+ \theta \dot h), h>, \quad \textit{ où } 0< \theta <1\]
		\item \[f(\bar{x}+h) = f(\bar{x}) + <\nabla f(\bar{x}), h> \int_0^1 (1-t) \dot <\nabla^2 f(\bar{x}+ t \dot h), h> dt\]
	\end{itemize}
	
		\subsubsection{rappel sur les fonction convexes}
		%--------------------%
		
		\begin{definition}
		Soit $C$ un convexe de $\R^n$, $f:C\ra \R$, $f$ est dite convexe ssi $\forall (x, x') \in C x C$ et $\forall \alpha \in ]0,1[$, on a $f(\alpha x + (1-\alpha) x') \leq \alpha f(x) + (1-\alpha) f(x')$.
		\end{definition}
		
		\begin{definition}
		$f$ est fortement convexe sur $C$ de module de forte convexité $c>0$ si $f(\alpha x + (1-\alpha) x') \leq \alpha f(x) + (1-\alpha) f(x') -\frac{1}{2} c \alpha (1-\alpha) ||x'-x||^2$.
		\end{definition}
		
		\begin{theoreme}
		Soit $f$ différentiable sur un ouvert $\Omega$ de $R^n$ et $C$ un convexe de $\Omega$.
		$f$ est convexe sur $C$ ssi $f(x) >= f(\bar{x}) + <\nabla f(\bar{x}), x-\bar{x}>$, $\forall (x, \bar{x}) \in C x C$.
		\end{theoreme}
		
		$y=f'(\bar{x}) \dot (x-\bar{x}) +f(\bar{x})$
		
		\begin{theoreme}
		Soit $f$ deux fois différentiable, sur un ouvert convexe $C$, alors :
		\begin{itemize}
			\item $f$ est convexe sur $C$ ssi $\nabla^2 f(x)$ est semi-définie positive.
			\item Si $\nabla^2 f(x)$ est semi-définie positive $\forall x \in C$ alors $f$ est strictement convexe sur $C$.
			\item $f$f est fortement convexe sur $C$ de module C ssi la plus petite valeur propore de $\nabla^2 f(x)$ est minorée par $c$, soit : <$\nabla^2 f(x) \dot \lambda, \lambda> >= c \dot --\lambda ||^2$, $\forall x \in C$ et $\forall \lambda \in R^n$.
		\end{itemize}
		\end{theoreme}
		
		
	\subsection{Exemples}
	%***********************%
	
		\begin{exemple}
		
		Soit $f:\Omega \ra \R^n$, ($\omega$ ouvert de $R^n$), $x|-> f(x) = A \dot x +b$ où $A \in Mn(R)$ et $b \in R^n$.
		
		Soit $\bar{x}) \in \Omega$, $f(\bar{x} +h) = A \dot (\bar{x}+h) + b = A \bar{x} + b + Ah = f(\bar{x}) + Ah + ||h|| \dot 0 = f(\bar{x}) + Ah + ||h|| \dot \epsilon (h)$, où $\epsilon (h) = 0$.
		
		Donc $f$ est différentiable en $\bar{x}$ et $\nabla f(\bar{x}) = A$.
		\end{exemple}
		
		\begin{exemple}
		
		Soit $\Phi : \Omega \; x \; \Omega \ra \R$ bilinéaire et symétrique, $f: \omega \R$, $x \mapsto f(x) = \Phi(x, x)$.
		Soit $\bar{x} \in \Omega$, $f(\bar{x}+h) = \Phi(\bar{x}+h, \bar{x}+h) = Phi(\bar{x}, \bar{x}) + \Phi(\bar{x}, h) + \Phi(h, \bar{x}) + \Phi(h, h) = f(\bar{x}) + 2 \dot \Phi(\bar{x}, h) + \Phi(h, h)$.
		
		Reste à montrer que $\Phi(h, h) = ||h|| \dot \epsilon (h) \Leftrightarrow \epsilon (h) = \frac{ \Phi(h, h) }{ ||h|| }$, avec $\lim{h \rightarrow 0} \epsilon (h) = 0$, \textit{ie} $\Phi(h, h) = o(h)$
		
		Comme $\Phi$ est bilinéaire et continue (car $dim < 2$) on a : $| \Phi (x, x) \leq c \dot ||x||^2$.
		$| \epsilon (h) | = \frac{ ? }{ ? } \leq c \frac{ ||h||^2 }{ ||h|| } = c ||h|| \rightarrow_{h \rightarrow 0} 0$. %TODO
		
		Donc f est différentiable en $\bar{x}$ et $<\nabla f(\bar{x}), h> = 2 \Phi( \bar{x}, h)$.
		\end{exemple}
		
		\begin{exemple}
		
		$f(x) = \frac{1}{2} <Ax, x> - <b, x>$ où $b \in R^n$ et $A \in Mn(R)$ symétrique ; étudier la différentiabilité de $f$.
		
		D'après le exemples 1 et 2, $f$ est différentiable et $\nabla f(\bar{x}) = A \bar{x} - b$, (où $<\nabla f(\bar{x}), h> = (A \bar{x} -b, h>$)
		
		Remarque : Si $A$ n'est pas symétrique, alors $\nabla f(\bar{x}) = \frac{1}{2} (A+A^t) \bar{x} - b$.
		\end{exemple}
		
		\begin{exemple}
		
		Soit $a \in R^n$ et $V$ un voisinage de $a$, soit $f:V \R$ différentiable en $a$, montrer que $\lim_{t \rightarrow 0} \frac{ f(a+th) - f(a) }{ t } = <\nabla f(a), h>$.
		
		Comme $f$ est différentiable en $a$, on a : $f(a+k) = f(a) + <\nabla f(a), k> + ||k|| \epsilon (k)$, avec $\lim_{k \rightarrow 0} \epsilon (k) = 0$.
		
		En particulier pour $k=th$, on a : $f(a+k) - f(a) = <\nabla f(a), th> + |t| \dot ||k|| \dot \epsilon (th) =  t <\nabla f(a), h> + |t| \dot ||k|| \dot \epsilon (th)$, et $\frac{ f(a+th) - f(a) }{ t } = <\nabla f(a), h> + \frac{|t|}{t} \dot ||k|| \dot \epsilon (th) \Rightarrow \lim_{t \rightarrow 0} \frac{ f(a+th) - f(a) }{ t } = < \nabla f(a), h>$. \P
		
		Si $f$ est telle que $\lim_{t \rightarrow 0} \frac{ f(a+th)-f(a) }{t}$ existe et vaut $l$, on dit que f admet une dérivé directionnelle en $a$ dans la direction $h$. On note cette dérivé $f'(a, h) = l$.
		
		On a vu que si $f$ est différentiable en $a$, alors $f'(a, h)$ existe (attention : la réciproque est fausse).
		\end{exemple}
		
		\begin{exemple} 
		
		$f(x, y) = \frac{x |y|}{\sqrt{x^2+y^2}}$, si $(x, y) \ne (0, 0)$
		
		$f(0, 0) = 0$
		
		$f$ admet des dérivés directionnelles en (0, 0) de direction $(h, k)$ quelconques.
		
		\[ \lim_{t \rightarrow 0} \frac{ f(0, t(h, k)) - f(0) }{t} = \lim \frac{ t(h, k)}{t} = \lim \frac{1}{t} \frac{t |t| h |k|}{ \sqrt{t^2 (h^2+k^2)}} = \frac{h|k|}{\sqrt{h^2+k^2}} \]
		
		Donc $f'(0, (h, k))$ existe ($h\ne0$ et $k\ne0$), cependant $f$ n'est pas différentiable en $0$.
		
		Si $f$ était différentiable en 0, alors on aurait : \[ f(h, k) = < \nabla f(0), (h, k)> + ||(h, k)|| \epsilon (h, k) \] 
		avec $\lim_{||(h, k)|| \rightarrow 0} \epsilon (h, k) = 0$
		\[ = \frac{ \partial f}{\partial x} (0, 0) h + \frac{ \partial f}{\partial y} (0, 0) k + o(||(h, k)||) \]
		
		Or $ \frac{ \partial f}{\partial x} (0, 0) = f' ((0, 0), e_1) = 0$ où $e_1 = (1, 0)$, et $ \frac{ \partial f}{\partial y} (0, 0) = 0$.
		
		On aurait alors $f(h, k) = o(||(h, k)||) = ||(h, k)|| \epsilon (h, k)$.
		
		Question : A-t-on $\lim_{(h, k) \rightarrow (0, 0)} \epsilon (h, k) = 0 $ ?
		
		$\lim_{(h, k) \rightarrow (0, 0)} \epsilon (h, k) = \lim_{(h, k) \rightarrow (0, 0)} \frac{f(h, k)}{||(h, k)||} = \lim_{(h, k) \rightarrow (0, 0)} \frac{h|k|}{h^2+k^2}$ qui n'existe pas.
		
		En effet, $\lim_{(h, k) \rightarrow (0, 0)} \frac{h|k|}{h^2+k^2} = \lim_{h=k, h \rightarrow 0} \frac{h|h|}{2h^2}$, $h=k \rightarrow 0^+$,  $h=k \rightarrow 0^-$.
		
		\[ \lim_{h \rightarrow 0^+} f(h, h) = \frac{1}{2} \ne -\frac{1}{2} = \lim_{h \rightarrow 0} \frac{h|h|}{2h^2} \]
		
		Donc $f$ n'est pas différentiable en $(0, 0)$.
		\end{exemple}
		
		\begin{exemple}
		
		Soit $u: \omega \R$ différentiable sur $\Omega$, soit $f: \omega \R$, $ x \mapsto f(x) = (u(x))^2$.
		
		$f$ est elle différentiable sur $\Omega$ ? Su oui, que vaut $ \nabla f(x)$ ?
		
		$a \in \Omega$, $u(a+h) = u(a) + <\nabla u(a), h> + o(h)$
		
		$f(a+h) = (u(a+h))^2 = (u(a) + <\nabla u(a), h> + o(h))^2 = u^2(a) + 2u(a)<\nabla u(a), h> +(<\nabla u(a), h>)^2 + \dots = f(a) + <2u(a)\nabla u(a), h> + o(h)$
		
		$f$ est différentiable et $\nabla f(a) = 2u(a) \nabla u(a)$.
		
		Cauchy-Schwartz : $|<\nabla u(a), h>| \leq ||\nabla u(a)|| \cdot ||h|| \Rightarrow |<\nabla u(a), h>|^2 \leq ||\nabla u(a)||^2 \cdot ||h||^2 \Rightarrow \frac{ (<\nabla u(a), h>)^2 }{||h||} \leq ||\nabla u(a)||^2 \cdot ||h|| \rightarrow_{||h|| \rightarrow 0} 0 \Rightarrow (< \nabla u(a), h>)^2 = o(h)$
		\end{exemple}
		
\section{Optimisation linéaire}
%=============================%
		
		
\section{Optimisation sans contraintes}
%=====================================%

	\subsection{1ère partie}
	
		\subsubsection{Condition d'optimalité}
		
		Soit $f : [a, b] \R$ continue et dérivable sur $[a,  b]$.
		Soit $\bar{x} \in [a, b]$ tel que $\bar{x}$ réalise un minimum local de $f$.
		
		\begin{theoreme} (condition d'optimalité du 1er ordre)
		
		Avec les hypothèses vues plus haut, on a $f'(x) = 0$.
		\end{theoreme}
		
		\textbf{Preuve} (faite)
		
		Attention : $f'(x) = 0$ n'est qu'une condition nécessaire, elle n'est pas suffisante ! (ex : $f(x) = x^3$)
		
		\begin{theoreme} (Weierstrass) :
		
		Si $f : [a, b]$ est continue, alors $f$ atteint ses bornes dans $[a, b]$, \textit{ie} il existe $x_1$ et $x_2$ $ \in [a, b]$ tels que $\displaystyle f(x_1) = \inf_{x \in [a, b]} f(x) = \min_{[a, b]} f$ et $\displaystyle f(x_2) = \sup_{x \in [a, b]} f(x) = \max_{[a, b]} f$ \\
		\end{theoreme}
		
		\textbf{Preuve} (déjà faite).\\
		
		\begin{definition}
		
		Soit $f : R \R$, on dit que $f$ est c\oe rcive si \[ \lim_{|x| \rightarrow \infty} f(x) = + \infty \]
		\end{definition}
		
		(ex : $|f(x)| \geq c|x|^\alpha$, avec $\alpha > 0$)
		
		Soit $f : R \R$ continue et c\oe rcive, alors $f$ atteint un minimum global.\\
		
		\textbf{Rappel} : $\displaystyle \lim_{x \rightarrow x_0} = l$ ssi ($\forall \epsilon >0$) $\exists \alpha >0$ tel que $|x-x_0| \Rightarrow |f(x) - l| < \epsilon$\\
		
		\textbf{Preuve} :
		
		Comme $\displaystyle \lim_{|x| \rightarrow + \infty} = +\infty$, on a pour tout $M>0$ il existe $\alpha >0$ tel que $|x|>\alpha \Rightarrow f(x)>M$
		Doc $f$ est minorée par $M$ sur $]-\infty, \alpha[ \cup ]-\alpha, +\infty[$. Par ailleurs, $f$ est continue sur le fermé borné $]-\alpha, \alpha[$.
		Cela implique (Weiestrass) que $f$ est bornée sur $]-\alpha, \alpha[$ et $y$ atteint ses bornes.
		Il existe $ \bar{x} \in ]-\alpha, \alpha[$ tel que $\displaystyle f(\bar{x}) = \inf_{\bar{x} \in ]-\alpha, \alpha[} f(x)$
		
		Si $\displaystyle \inf_{\bar{x} \in ]-\alpha, \alpha[} f(x) \leq M$ alors $\displaystyle f(x) > \inf_{x \in R} f(x) = \inf_{\bar{x} \in ]-\alpha, \alpha[} f(x)$
		
		Si $\displaystyle \inf_{\bar{x} \in ]-\alpha, \alpha[} f(x) > M$ alors sur $]-\alpha, \alpha[$ on a $\displaystyle f(x) \geq \inf_{\bar{x} \in ]-\alpha, \alpha[} f(x) > M$
		
		(Choisir $M = f(x_0)$ avec $x_0 \in R$, on a : $\inf_{]-\alpha, \alpha[} f(x) > f(x_0) > f(x)$, $\forall |x| > \alpha$)
		
		\subsubsection{Conditions d'optimalité du second ordre}
		
		Soit $f : [a, b] \R$ continue sur $[a, b]$ et dérivable sur $]a, b[$.
		
		\begin{theoreme} (conditions nécessaire) : 
		
		Soit $\bar{x} \in ]a, b[$ tel que $\bar{x}$ réalise un minimum local de $f$. ALors $f'(\bar{x}) = 0$ et $f''(\bar{x}) \geq 0$.
		\end{theoreme}
		
		En effet, $f$ admet un minimum local en $\bar{x} \Rightarrow \exists \alpha >0$ tel que $|x-\bar{x}|<\alpha \Rightarrow f(x) \geq f(\bar{x})$.
		
		D'après Taylor-Young, on a : $\displaystyle f(\bar{x}+h) -f(\bar{x}) = \frac{1}{2} h^2 f''(\bar{x}) + o(h^2)$, $\forall |h|<\alpha$
		
		On divise par $h^2 \ne 0$, et on obtient : $\displaystyle 0 \leq \frac{f(\bar{x}+h) - f(\bar{x}) }{ h^2 } = \frac{1}{2} f''(\bar{x}) + o(1) $, et $o(1) \sim \epsilon(h)$ tel que $\lim_{h \rightarrow 0} \epsilon(h) = 0$.
		Prendre la limite quand $h\rightarrow 0$ : $\frac{1}{2} f''(\bar{x}) \geq 0$.
		
		\begin{theoreme} (condition suffisante) :
		
		$f : [a, b] \R$ continue et deux fois dérivable sur $]a, b[$. Soit $\bar{x} \in ]a, b[$ tel que $f'(\bar{x}) = 0$ et $f''(\bar{x}) \geq 0$. ALors $\bar{x}$ réalise un minimum local strict de $f$ sur $[a, b]$.
		\end{theoreme}
		
		En effet,par Taylor-Young, on a: $f(\bar{x}+h) -f(\bar{x}) = \frac{1}{2} h^2 f''(\bar{x}) + h^2 \epsilon(h)$ et $signe( f(\bar{x}+h) - f(\bar{x}) ) = signe( \frac{1}{2} f''(\bar{x}) + \epsilon(h))$.
		
		Or $\displaystyle \epsilon(h) \longrightarrow_{h\rightarrow 0} 0$, donc pour $h$ suffisamment petit, on a : 
		\[ \frac{1}{2} f''(\bar{x}) + \epsilon(h) >0\]
		\[ \epsilon(h) \longrightarrow_{h\rightarrow 0} 0 \Rightarrow |\epsilon(h)| < \frac{1}{4} f''(\bar{x}) \textit{ pour h petit}\]
		\[-\frac{1}{4} f''(\bar{x}) < \epsilon(h) < \frac{1}{4} f''(\bar{x})\]
		\[0< \frac{1}{4} f''(\bar{x}) < \frac{1}{4} f''(\bar{x}) + \epsilon(h) \]
		Or $f(\bar{x}+h) - f(\bar{x}) >0$ pour $h$ suffisamment petit, $ \Rightarrow \bar{x}$ réalise un minimum local strict.
		
		\subsection{2ème partie : fonction de plusieurs variables}
		
		\begin{theoreme}		
		Soit $f : B(\bar{x}, \alpha) \rightarrow \R$,  (où $B(\bar{x}, \alpha) \subset \R^n$ est la boule de rayon $\alpha$ centrée sur $\bar{x}$) continue et différentiable sur $B(\bar{x}, \alpha)$.
		
		On suppose que $f$ admet un minimum local en $\bar{x}$, alors $\nabla f(\bar{x}) = 0$ (condition d'optimalité du premier ordre.
		\end{theoreme}
		
		\textbf{Preuve} (en exercice)
		
		Par Taylor-Young on a : $f(\bar{x}+h)-f(\bar{x}) = < \nabla f(\bar{x}) ,h> + ||h|| \epsilon(h)$ avec $\lim_0 \epsilon = 0$.
		
		%TODO
		
		Le théorème de Weierstrass reste vraie dans $\R^n$.
		$f:X\rightarrow \R$ où $X$ est un compact de $R^n$.
		Si $f$ est continue sur $X$, alors $f$ y atteint ses bornes.
		
		\begin{theoreme} : 
		Soit $f:R^n \R$ continue et c\oe rcive, alors $f$ admet un minimum global.
		\end{theoreme}
		
		\begin{theoreme} (Condition Nécessaire)
		Soit $f : B(\bar{x}, \alpha) \subset \R^n \R$ continue et deux fois différentiable.
		
		Si $\bar{x}$ réalise un minimum local, alors $\nabla f(\bar{x}) = 0$ et $\nabla^2 f(\bar{x})$ est semi-définie positive.
		\end{theoreme}
		
		\begin{theoreme} (Condition Suffisante)
		Soit $\bar{x}$ tel que $\nabla f(\bar{x}) = 0$ et $\nabla^2 f(\bar{x})$ soit définie positive, alors $\bar{x}$ réalise un minimum local de $f$.
		\end{theoreme}
		
		\begin{exemple}
		On veut minimiser la fonction quadratique $f(x) = <Ax, x> +2<b, x> +c$ où $A$ est symétrique définie positive $\forall x \in \R^n$.
		($\nabla f(\bar{x}) = 2A\bar{x} +2b$)
		
		$f$ est un polynôme de degré 2 en $x_i$ $\Rightarrow f$ est continue sur $\R^n$.
		
		$f$ est c\oe rcive et donc 
	 	$ f(x) = <Ax, x>  +2<b, x> +c \geq \lambda ||x||^2 + 2<b, x> +c \geq \lambda ||x||^2 - 2||b|| \cdot ||x|| +c $
		
		(car par Cauchy-Schwartz : $|<b, x>| \leq ||b|| \cdot ||x||$)
		
		où $\lambda$ est la plus petite des valeurs propres de $A$, et comme $A$ est symétrique définie positive, on a $\lambda >0$.
		
		Or $\displaystyle \lim_{||x|| \rightarrow +\infty}( \lambda ||x||^2 -2||b|| \cdot ||x|| +c) = +\infty$.
		$\displaystyle \Rightarrow \lim_{||x|| \rightarrow +\infty}f(x) = +\infty \Rightarrow f$ est c\oe rcive.
		
		Si $f$ est continue et c\oe rcive, alors $f$ admet un minimum global atteint en au moins un $\bar{x}$ solution de $\nabla f(\bar{x}) = 0$.
		
		$\nabla f(\bar{x}) = 2A\bar{x} +2b = 0 \Rightarrow \bar{x} = A^{-1} b$, et comme $A$ est symétrique définie positive, $\Rightarrow A$ est inversible.
		
		$f(x) = <Ax, x> +2<b, x> +c$ est strictement convexe $\Rightarrow$ il y a unicité de la solution.
		\end{exemple}
		
		\begin{exemple} (droite de régression)
		
		Soient $N$ points $\{(x_i, y_i), 1\leq i \leq N\}$ d'abscisses strictement croissantes.
		
		Déterminer une droite d'équation $y=ax+b$ telle que la quantité $\displaystyle f(a, b) = \sum_{i=1}^N (y_i-(ax_i+b))^2$ soit minimale.
		
		$f$ est continue et différentiable en tout $(a, b) \in \R^2$.
		On veut que $\nabla f(a, b) = 0$.
		
		Notations :
		$\displaystyle \bar{x} = \frac{1}{N} \sum_{i=1}^N x_i$, 
		$\displaystyle \bar{y} = \frac{1}{N} \sum_{i=1}^N y_i$, 
		$\displaystyle s_2 = \frac{1}{N} \sum_{i=1}^N x_i^2$, 
		$\displaystyle s_1 = \frac{1}{N} \sum_{i=1}^N x_i y_i$.
		
		\begin{eqnarray*}%[\{]
 \frac{\partial f}{\partial a} = \sum_{i=1}^N 2(y_i-(ax_i+b)) \\ \frac{\partial f}{\partial b} = \sum_{i=1}^N 2(y_i-(ax_i+b)) \end{eqnarray*}

		\end{exemple}
		
		
\section{Optimisation avec contraintes \\ Conditions de Lagrange \\ Conditions de Karush-Kuhn-Tucker }
%=====================================%

\subsection{Optimisation avec contraintes de type égalité - Condition de Lagrange}
Soient $f : \R^n \rightarrow \R$, non nécessairement linéaire et K un sous ensemble de $\R^n$.\\
Nous nous intéressons au problème d'optimisation suivant :
 \[ (\mathcal{P})\begin{cases}\min f(x) \\ x \in K \end{cases} \]

%TODO

Nous nous intéressons au problème d'optimisation suivant : \[ (P) : \min f(x), x \in K \]

Il s'agit de trouver $\bar{x} \in K$ tel que \[ f(\bar{x}) = \inf f(x) \]

% TODO

Généralement $K$ est définie par des fonctions $h_i$, $1 \leq i \leq p$, de la manière suivante : \[ TODO \]

On parle de contraintes de type égalité.

%TODO

\begin{exemple}
	Considérons le problème de minimisation, avec une seule contrainte, suivant : \[ TODO \]	
	
	L'ensemble des contraintes est le cercle de centre 0 et de rayon $\sqrt 2$ \[ K = C(0, \sqrt(2)) \].
	
	On considère les droites d'équation \[  \]
	
	Les droites $(\Delta_a)$ se déplacent vers le \og sud-ouest \fg. $f$ atteint donc son minimum en \[ \bar{x} = (-1, -1) \]
	
	En $\bar{x}$, on a $\displaystyle \nabla f = ...$ et $ $ où $h(x) = x_1^2 + x_2^2 - 2$
	
	On remarque que $ \nabla h( \bar{x} ) = -2  \nabla f( \bar{x} ) $
	
	
\end{exemple}

%%%%%%%%%%%%%%%%%%%%%%% TODO %%%%%%%%%%%%%%%%%%%%%%

\begin{remarque} (sur la droite de régression)

	Quand on veut approcher un nuage de points $(x_i, y_i)_{1 \leq i \leq n}$ par une droite $y = ax +b$, la mesure naturelle serait la somme des valeurs absolues des résidus $r_i = y_i -(ax_i+b)$ mais il est plus commode de considérer la somme des $r_i^2$.

	\[ 1 \leq \frac{ |r_1| + \dots + |r_n| }{ (r_i^2 + \dots + r_n^2)^{\frac{1}{2}} } \leq \sqrt{n} \]
	
	Les deux erreurs ne sont proches que si n est petit.
\end{remarque}

\begin{exemple}
	Soient $1 < p \leq q$,
	\begin{align*}
	f : \R^n & \rightarrow  \R \\
	x & \mapsto \sum_{i=1}^n (x_i)^p
	\end{align*}
	\begin{align*}
	g : \R^n & \rightarrow  \R \\
	x & \mapsto \sum_{i=1}^n (x_i)^q
	\end{align*}
	\[ S = \{ x \in \R, \quad g(x) = 1 \} \]
	
	Étudier le problème suivant :
	\[ \mathcal{P} \begin{cases} f(x), \\ x \in S \end{cases} \]
	
	
\end{exemple}

\subsection{Condition de type inégalité (condition de Karush Kuhn Tucker)}

\begin{exemple}
	Reprenons le problème $(P1)$ avec une contrainte d'inégalité.
	\[ (\mathcal{P}) : \min f(x) = x_1 + x_2 \]
	sachant que $g(x) = x_1+x_2 -2 \leq 0$.
	
	$S = \{ g(x) = 0 \}$ est le cercle $C(0, \sqrt{0})$ qui est compact. $f$ étant continue, $f$ atteint son minimum en un point $\bar{x} \in S$
	
	$\bar{x} \in S$(avec un rond : intérieur de S) sinon $\nabla f$ serait nul, ce qui n'est pas le cas.
	
	La condition de Lagrange dit : $\exists \lambda \in \R$ tel que : \[ \nabla f + \lambda \cdot \nabla h = 0 \]
	$\Rightarrow \bar{x} = (-1, -1) et \lambda = \frac{1}{2}$ ou $\bar{x} = (1, 1) et \lambda = -\frac{1}{2}$
	
\end{exemple}
	
	\subsubsection{CN d'optimalité : règle des multiplicateurs de KKT}
	
	On considère le problème suivant : \[ \begin{cases}\min f(x) \\ g_j(x) \leq 0 , \quad 1 \leq j \leq p \end{cases} \]
	
	On suppose que :
\begin{itemize}
	\item $\bar{x}$ est un point admissible
	\item $f:B(\bar{x},\alpha)$ est différentiable
	\item toutes les fonctions $(g_j)_j$ sont continuement différentiables en $\bar{x}$
\end{itemize}

On désigne par $J(x)$ l'ensembles des indices des contraintes actives (saturées) au point $\bar{x}$, ie $J(x) = \{ j \in \{ 1, \dots, p \}, \quad g_j(\bar{x}) = 0 \} $.

On ajoute l'hypothèse suivante, dite de qualification des contraintes : les gradients des contraintes actives au point $\bar{x}$ : $\nabla g_j(\bar{x}), {j\in J(\bar{x})}$ sont linéairement indépendants.

\begin{theoreme} (CN de KKT)
	
	Si $\bar{x}$ réalise un minimum local de $f$, alors il existe un vecteur $\mu \in \R_+^p$, $\mu \ne 0$ de multiplicateur positif tel que \[ \begin{cases} \displaystyle \nabla f(\bar{x}) + \sum_{j=1}^p \mu _j \nabla g_j (\bar{x}) = 0 \quad (i) \\ \mu _j g_j(\bar{x}) = 0, \quad \forall i \leq j \leq p \quad (ii) \end{cases} \]
	
\end{theoreme}

\begin{remarque}
La condition $(ii)$ signifie que si $j \not \in J(\bar{x})$ alors $\mu_j = 0$.

La condition de KKT devient \[ \nabla f(\bar{x}) + \sum_{j \in J(x)} \mu _j \nabla g_j (\bar{x}) = 0 \]
\end{remarque}

\begin{theoreme} (CS d'optimalité de KKT pour les problèmes convexes)

Supposons $f$ et $g_j$ convexes définies sur un convexe $C$ de $\R^n$. Si les conditions de KKt sont satisfaites en $\bar{x}$, alors $\bar{x}$ réalise un minimum global.
\end{theoreme}

\subsection{Optimisation avec contraintes mixtes}

Considérons le problème d'optimisation suivant avec les deux types de contraintes : \[ (\mathcal{P}) \begin{cases}\min f(x) \\ h_i(x) = 0 , \quad 1 \leq j \leq n \\ g_j(x) \leq 0 , \quad i \leq j \leq p \end{cases} \]

On désigne par $J(\bar{x})=\{j \in \{1, \dots , p\}; g_j(\bar{x}) = 0\}$.
On ajoute les hypothèses suivantes de qualification de contraintes :
\begin{itemize}
	\item[1)] $\exists \lambda_0 \in \R^n$ tel que :
	\begin{itemize}
		\item $<\nabla h_i(\bar{x}), \lambda_0> \; = 0, \quad \forall i \in \{ 1, \dots , n \}$
		\item $<\nabla g_j(\bar{x}), \lambda_0> \; < 0, \quad \forall j \in \{ 1, \dots , p \}$
	\end{itemize}
	\item[2)] $\nabla h_1(\bar{x}), \dots, \nabla h_n(\bar{x})$ sont linéairement indépendants
\end{itemize}

\begin{theoreme} (CN d'optimalité de KKT)

Si $\bar{x}$ réalise un minimum local de $f$, alors $\exists \lambda \in \R^n \text{  et } \mu \in \R^p$ tel que :
	\begin{itemize}
		\item[(i)] $\displaystyle \nabla f(\bar{x}) + \sum_{i=1}^n \lambda _i \nabla h_j (\bar{x}) + \sum_{j=1}^p \mu _j \nabla g_j (\bar{x}) = 0$
		\item[(ii)] $ \mu _j \geq 0 \quad \forall j \in \{ i, \dots , p \}$
		\item[(iii)] $ \mu _j \cdot g_j(\bar{x}) = 0 \quad \forall j \in \{ 1, \dots , p \}$
	\end{itemize}
\end{theoreme}

\begin{theoreme} (CS d'optimalité dans le cas convexe)

Supposons que $f$ et $g_j$ soient convexes et que les $h_i$ soient affines. ALors si les conditions de KKT sont satisfaites au point $\bar{x}$, alors ce point est un minimum de $f$.
\end{theoreme}
\begin{exemple} %1

	Soient $ \displaystyle f(x) = \sum_{i=1}^n |x_i|^p $, 
	$ \displaystyle g(x) = \sum_{i=1}^n |x_i|^q $, et
	$ \displaystyle C = \{ x \in \R ; g(x) = 1 \} $
	
	%\[ \min TODO \]
	
	Comme $p>1$ et $q>1$, les fonctions $f$ et $g$ sont différentiables sur $\R^n$, et on obtient :\\ $\displaystyle (\nabla f)_i = \begin{cases} p |x_i|^{p-1} \quad \text{ si }x_i\geq 0 \\  -p |x_i|^{p-1} \quad \text{ si } x_i < 0  \end{cases}$ 
	
	La condition de Lagrange s'écrit : $\exists \lambda \in \R$ tel que 
	\[ \nabla f(\bar{x}) + \lambda \nabla g(\bar{x}) = 0 \]
	\[ (\nabla f(\bar{x}))_i + (\lambda \nabla g(\bar{x}))_i = 0 , \quad \forall i \in \{ 1, \dots, n \} \]
	\[ p|x_i|^{p-1} + \lambda q|x_i|^{q-1} = 0 , \quad \forall i \in \{ 1, \dots, n \} \]
	%TODO	La méthode de ré-injection tourne en rond.
	$ \Rightarrow \lambda = - \frac{p}{q}|x_i|^{p-q} $ existe et est unique (au point $\bar{x}$). On en déduit que les valeurs absolues de tous les $x_i$ sont égales.
	Notons $k$ le nombre des $x_i$ non nuls ($k \geq 1$ car sinon $g(x) = 0$ et $\bar{x} \not \in C$)
	$f(\bar{x}) = k \alpha ^p$ où $\alpha = |x_i|$.
	
	On veut maximiser $f$, on sait que $\bar{x} \in C \Rightarrow g(\bar{x}) = 1 \Rightarrow \sum |x_i|^q = 1 \Rightarrow k |x_i|^q = 1 \Rightarrow k \alpha^q = 1 \Rightarrow \alpha = k^{-\frac{1}{q}} \Rightarrow f(\bar{x}) = k \alpha^p = k \cdot k^{-\frac{p}{q}} = k^{1-\frac{p}{q}} $
	
	Or $1-\frac{p}{q} \geq 0$, donc $f(\bar{x})$ est maximal pour $k=n$, $f(\bar{x}) = n^{1-\frac{p}{q}}$ et $\bar{x} = (^+_- n^{-\frac{1}{q}}, \dots, ^+_- n^{-\frac{1}{q}})$.
	
\end{exemple}

\begin{exemple}


 	% 2
	Soit $A$ un matrice de $Mn(\R)$. Notons $\lambda _1$ sa plus petite valeur propre. On suppose que $\lambda _0 \leq 0$. On considère le problème d'optimisation suivant : \[ \begin{cases}\min f(x) = <Ax, x> \quad 0 \not \in sp(A)\\ x \in C = \{ x \in \R, ||x||^2 \leq 1 \} \end{cases} \]
	
	$C$ est fermé borné donc $C$ est compact ;
	$f$ est continue sur $\R^n$, donc $f$ atteint son minimum en un point $\x$ de $C$.
	
	% 2 %
	Posons $ g(x) = ||x||^2 - 1 $, $g$ est différentiable sur $\R^n$ tout entier.\\
Si $\bar{x}$ réalise le minimum de $f$ alors $\bar{x}$ est sur le bord de $C$. Prouvons le par l'absurde.\\
On suppose que $\bar{x} \in \textit{ l'intérieur de }C $.
Cela implique que : \[ \exists \alpha>0 \text{ tel que la boule } B(\bar{x},\alpha)  \]
\[ f : B(x,\alpha) \rightarrow R \]
$f$ est différentiable sur un voisinage de $\bar{x}$ et atteint son minimum en $\bar{x}$.
	
	$\Ra \nabla f(\x) = 0$
	Or $\nabla f(\x) = \not 2A\x$
	on aurait $2A\x = 0 \Lra A\x = 0$ qui est interdit car $0 \not \in sp(A)$
	Donc $\x \in \partial C \text{(bord de C)} \Ra g(\x) = 0 \Ra$ la contrainte est active en $\x$.
	
	% 4 %
	\[ \exists \lambda > 0 \text{ tel que } \nabla f(\bar{x}) + \lambda.\nabla g(\bar{x}) = 0 \]
\[ \Rightarrow 2A\bar{x} + 2\lambda\bar{x} = 0 \]
\[ \Rightarrow A\bar{x} = - \lambda\bar{x}\]

Cela implique que $\bar{x}$ est un vecteur propre associé à la valeur propre $-\lambda$. $\bar{x}$ est à chercher dans l'ensemble des vecteurs propres unitaires associés à des valeurs propres négatives.
	
	$f(\x) = <A\x, \x> = <-\lambda \x, \x> = -\lambda <\x, \x> = - \lambda$ \\
	Donc pour minimiser $f$f, on prend $\x$, un vecteur propre unitaire associé à $\lambda_1$.\\
	On aura au préalable montré que la contrainte est qualifiée en $\x$ : on a $\nabla g(\x) = 2\x \ne 0$ (car $||\x||=1$).
	
\end{exemple}

\section{Algorithmes de minimisation sans contraintes}
\subsection{Résolution approchée d'équations non linéaires}
\subsection{Méthode de Newton}



\subsection{Méthode de la sécante}
\subsection{Méthodes de descente}
\subsection{Algorithme du gradient à pas fixe}
\subsection{Algorithme du gradient à pas optimal}


% 28/03/2011




% 01/04/2011

\begin{proposition}

Soit $A \in Mn(\R)$ S.D.P., $b\in\R^N$ et $:\R^N\ra\R$, $x\mapsto f(x)=\frac{1}{2}<Ax,x>-<b,x>$.

Soit $(x^{(n)})$ définie positive comme suit :

\textsc{Initialisation :}
\[x^{(0)}\in\R\]

\textsc{Itération :}
$x^{(n)}$ connu, on pose $x^{(n+1)}=x^{(n)}+\rho_n w^{(n)}$ où :
\begin{enumerate}
\item $w^{(n)} \ne 0$ est une direction de descente stricte de $f$ en $x^{(n)}$.
\item $\rho_n$ est optimal dans la direction $w^{(n)}$.
\end{enumerate}

Si la famille $(w^{(0n)}, w^{(1)}, \dots, w^{(N-1)})$ est une famille A-conjugué, alors on a :
\[ x^{(N)}=\bar{x} \textit{ avec } A\bar{x}=b \]
\[ f(\bar{x})=\inf_{\R^N} f \]

\end{proposition}

\textbf{Les grandes lignes de la preuve}\\
Tout revient à prouver que :
\[<Ax^{(N)}-b,w^{(p)}> = 0, \forall p \in \{0,1,\dots,N-1\}\]

Cas $p=N-1$, on doit montrer que :
\[<Ax^{(N)}-b,w^{(N-1)}> = 0\]
mais \[ x^{(N)}=x^{(N-1)}+\rho_{N-1}w^{(N-1)} \]
avec \[ \rho_{N-1} \textit{ optimal dans la direction } w^{(N-1)} \] 
\[ Ax^{(N)}-b=\nabla f(x^{(N)}) = \nabla f(x^{(N-1)}+\rho_{N-1}w^{(N-1)}) \]
Donc \[ <Ax^{(N)}-b,w^{(N-1)}> = <\nabla f(x^{(N-1)}+\rho_{N-1}w^{(N-1)}), w^{(N-1)}>=\phi '(\rho_{N-1}) \]
Où \[ \phi (t)=f(x^{(N-1)}+tw^{(N-1)}) \]
\[ \phi'(t)=<\nabla f(x^{(N-1)}+tw^{(N-1)}, w^{(N-1)}>\]
\[ (\rho_{N-1} \textit{ optimal }) \Ra \phi'(\rho_{N-1})=0 \]
Donc \[ <Ax^{(N)}-b,w^{(N-1)}>=0 \]

Cas $0\leq p \leq N-1$, montrons que :
\[ <Ax^{(N)}-b,w^{(p)}>=0 \]
or \[ Ax^{(N)}-b=A(x^{(N-1)}+\rho_{N-1}w^{(N-1)})-b \]
\[ =Ax^{(N-1)}-b+\rho_{N-1}Aw^{(N-1)} \]
\[ =A(x^{(N-2)}+\rho_{N-2}w^{(N-2)})-b+\rho_{N-1}Aw^{(N-1)} \]
\[ =(Ax^{(N-2)}-b)+\rho_{N-1}Aw^{(N-1)}+\rho_{N-2}Aw^{(N-2)} \]
Par itération on obtient :
\[ Ax^{(N)}-b=Ax^{(p+1)}-b+\sum_{j=p+1}^{N-1}\rho_jw^{(j)} \]
Donc \[ <Ax^{(N)}-b,w^{(p)}>=<Ax^{(p+1)}-b,w^{(p)}>+\sum_{j=p+1}^{N-1}\rho_j<Aw^{(j)},w^{(p)}>=0 \]
\[(\textit{cf plus haut avec }\phi), ((w^{(0)}, \dots, w^{(N-1)}) \textit{ est A-conjuguée}) \]
Donc $Ax^{(N)}-b$ est orthogonal à la famille $(w^{(0)}, \dots, w^{(N-1)}$, or d'après la proposition précédente, la famille $(w^{(0)}, \dots, w^{(N-1)}$ est une base de $\R^N$ (car A-conjugué et contient $N$ vecteurs).

On en déduit que : $Ax^{(N)}-b$ est orthogonal à $\R^N$, $\Ra Ax^{(N)}-b=0 \Ra Ax^{(N)}=b \Ra x^{(N)}=\bar{x}$ (unicité de la solution de $Ax=b$ car $A$ SDP $\Ra A$ inversible).

\begin{remarque}
Cette proposition suggère le principe suivant : si on connait $x^{(0)}, x^{(1)}, \dots, x^{(n)}$ et $w^{(0)}, w^{(1)}, \dots, w^{(n)}$, on cherche $w^{(n)}$ de la manière suivante :
\begin{enumerate}
\item $w^{(n)}$ est une direction de descente de $f$ en $x^{(n)}$
\item $w^{(n)}$ est A-conjugué avec tous les $w^{(n)}, p<n$
\end{enumerate}
Avec un tel $w^{(n)}$, on pose $x^{(n+1)}=x^{(n)}+\rho_nw^{(n)}$ avec $\rho_N$ optimal dans la direction de descente $w^{(n)}$. On a $x^{(N)}=\bar{x}$ avec $A\bar{x}=b$.
\end{remarque}

\subsection{L'algorithme du gradient conjugué}

\[f:\R^N\ra \R \]
\[x \mapsto f(x)=\frac{1}{2}<Ax,x>-<b,x> \]

\textsc{Initialisation}

Soit $x^{(0)} \in \R$ et $r^{(0)}=b-Ax^{(0)}=-\nabla f(x^{(0)})$
\begin{enumerate}
\item si $r^{(0)}=0$, alors $Ax^{(0)}=b$ et $x^{(0)}=\bar{x}$, l'algorithme s'arrête ;
\item si $x^{(0)}\ne0$, alors on pose $w^{(0)}=r^{(0)}$ et on choisit $\rho_0$ optimal dans la direction $w^{(n)}$
\end{enumerate}
On pose $w^{(1)}=w^{(0)}+\rho_0w^{(0)}$.

\textsc{Iteration}

$1\leq n\leq N-1$, on suppose $x^{(0)},x^{(1)},\dots,x^{(n)}$ et $w^{(0)},w^{(1)},\dots,w^{(n)}$ connus, on pose $r^{(n)}=b-Ax^{(n)}$
\begin{enumerate}
\item si $r^{(n)}=0$, alors $Ax^{(n)}=b$ et $x^{(n)}=\bar{x}$, l'algorithme s'arrête ;
\item si $r^{(n)}\ne0$, alors on pose $w^{(n)}=r^{(n)}+\lambda_{n-1}w^{(n-1)}$ avec $\lambda_{n-1}$ tel que $<w^{(n)},Aw^{(n-1)}=0$ et on choisit $h$ optimal dans la direction $w^{(n)}$ et on pose $x^{(n+1)}=x^{(n)}+\rho_nw^{(n)}$
\end{enumerate}

\textbf{Preuve :} admise\\
La suite $x^{(n)} \ra \bar{x}=x^{(N)}$.

\begin{exemple}
Méthode de Polak-Ribière

C'est une méthode de gradient conjugué pour une fonction qui n'est pas nécessairement quadratique.
\[ f:\R^N\ra \R \in C^2\]
\[ \alpha |y|^2 \leq <\nabla^2f(x)y, y> \leq \beta |y|^2 \quad \forall x, y \in \R^N, \quad 0<\alpha\leq\beta \]

\begin{enumerate}
\item Montrer que $f$ est strictement convexe et c\oe rcive,
\item Montrer que $\sp(\nabla^2f)$ est borné.
\end{enumerate}

1) Soit $x$ et $y$ dans $\R^N$, d'après Taylor-Lagrange, on a : \[ f(y)=f(x)+<\nabla f(x),y-x>+\frac{1}{2}<\nabla^2 f(x+\theta(y-x))(y-x),y-x> \quad (0<\theta<1) \]
\[ f(y)\geq f(x)+<\nabla f(x),y-x>+\frac{\alpha}{2}|y-x|^2 \quad (*)\quad \Ra f \textit{ est fortement convexe}\]
On fixe $x$ et on prend la limite quand $||y|| \ra \infty$ dans $(*)$ : \[ \lim_{||y|| \ra \infty} f(y) \geq f(x)+\lim_{||y|| \ra \infty}(-|\nabla f(x)||y_x|+\frac{\alpha}{2}|y_-x|^2)=+\infty \]
\[ \Ra \lim_{||y|| \ra \infty} f(y) = +\infty \quad \Ra f \textit{ est c\oe rcive} \]

% 04/04
2) Montrer que le spectre de $\nabla^2f$ est borné.\\
Par hypothèse :
\[ \alpha|y|^2 \leq <\nabla^2 f(x)y,y> \leq \beta|y|^2, \quad \forall x, y\in\R^N \]
Doit $\lambda \in sp(\nabla^2f(y))$, soit $y\in\R^N$ le vecteur propre associé à la valeur propre $\lambda$. On a :$\nabla^2f(x)y=\lambda y$
\[ \alpha|y|^2 \leq |<\nabla^2 f(y),y>| \leq \beta|y|^2 \]
\[ \alpha|y|^2 \leq |<\lambda y,y>| \leq \beta|y|^2 \]
\[ \alpha|y|^2 \leq |\lambda| <y,y> \leq \beta|y|^2 \]
\[ \alpha|y|^2 \leq |\lambda| |y|^2 \leq \beta|y|^2 \quad (*)\]
Or $y\ne0$ (car $y$ est un vecteur propre), donc $(*) \Ra \alpha\le |\lambda|\leq \beta \Ra$ le spectre de $\nabla^2f(x)$ est borné.

\begin{remarque}
Comme $f\in C^2(\R^N-\R)$, la matrice hessienne $\nabla^2f(x)$ est symétrique (Lemme de Schwartz). Or toute matrice symétrique de $Mn(\R)$ est diagonalisable sur $\R \Ra \R^N$ admet une base de vecteurs propres de $\nabla^2f(x)\Ra$ un vecteur propre $y$ est un élément de $\R^N$.
\end{remarque}

\[ \Ra\exists!\; \bar{x} \textit{ tel que } \quad f(\bar{x})=\inf_{x\in\R^N}(x) \]

\end{exemple}

\textsc{Initialisation}

$x^{(0)}\in\R^N$, on pose $g^{(0)}=-\nabla f(x^{(0)})$
\begin{itemize}
\item $g^{(0)}=0$, on s'arrête,
\item $g^{(0)}\ne0$, on pose $w^{(0)}=g^{(0)}$ et $x^{(1)}=x^{(0)}+\rho_0w^{(0)}$ avec $\rho_0$ optimal dans la direction $w^{(0)}$
\end{itemize}

\textsc{Itération}

$x^{(n)}, w^{(n-1)}$ connus, on pose $g(n)=-\nabla f(x^{(n)})$
\begin{itemize}
\item si $g^{(n)}=0$, on s'arrête,
\item si $g^{(n)}\ne0$, on pose :
\end{itemize}
\[ \lambda_{n-1} = \frac{ <g^{(n)},(g^{(n)}-g^{(n-1)})> }{ <g^{(n-1)},g^{(n-1)}> }\]
\[ w^{(n)}=g^{(n)}+\lambda^{(n-1)}w^{(n-1)} \]
\[ x^{(n+1)}=x^{(n)}+\lambda_nw^{(n)} \quad \textit{ avec } \lambda_n \textit{optimal dans la direction de descente }w^{(n)} \]
Dans la suite, on suppose $g^{(n)}\ne0$.

2) Montrer que $<g^{(n+1)},w^{(n)}>=0$ et $<g^{(n)},g^{(n)}>=<g^{(n)},w^{(n)}>$.
On le fait par récurrence sur $n$ :

\textbf{Cas $n=0$ :}

$g^{(1)}=-\nabla f(x^{(1)})$, donc $<g^{(1)},w^{(0)}>=-<\nabla f(x^{(1)}),w^{(0)}>=-<\nabla f(x^{(0)}+\rho_0w^{(0)},w^{(0)}>=-\phi'(\rho_0)=0$ (car $\rho_0$ est optimal) ; et $<g^{(0)},g^{(0)}>=<g^{(0)},w^{(0)}>$ par définition.

On suppose la propriété vrai à l'ordre $(n-1)$ et on montre qu'elle l'est toujours à l'ordre $n$.

\textbf{Cas $n$ :}

$<g^{(n)},w^{(n)}>=<g^{(n)},g^{(n)}+\lambda_{n-1}w^{(n-1)}>=<g^{(n)},g^{(n)}>+\lambda_{n-1}<g^{(n)},w^{(n-1)}>=<g^{(n)},g^{(n)}>$ 

$<g^{(n+1)},w^{(n)}>=<-\nabla f(x^{(n+1)}),w^{(n)}>=<-\nabla f(x^{(n)}+\rho_nw^{(n)}),w^{(n)}>=0$ par optimalité de $\rho_n$.

3) On pose \[ J^{(n)}=\int_0^1 \nabla^2 f(x^{(n)}+\rho_nw^{(n)})dt \]
Montrer que \[ g^{(n+1)}=g^{(n)}-\rho_nJ^{(n)}w^{(n)} \] et que \[ \rho_n = \frac{ <-g^{(n)},w^{(n-1)}> }{ <J^{(n)}w^{(n)},w^{(n)}>},\quad \forall n\in N \]

$g^{(n+1)}-g^{(n)}=\nabla f(x^{(n)})-\nabla f(x^{(n+1)})$
Posons $\psi(t)=\nabla f(x^{(n)}+t\rho_nw^{(n)})$, $\psi'(t)=\nabla^2 f(x^{(n)}+t\rho_nw^{(n)})\rho_nw^{(n)}$, $\psi(1)-\psi(0)=\int_0^1 \psi'(t)dt \Ra \nabla f(x^{(n)}+\rho_nw^{(n)})-\nabla f(x^{(n)})=(\int_0^1\nabla^2 f(x^{(n)}+\rho_nw^{(n)}))\rho_nw^{(n)}==\rho_nJ^{(n)}w^{(n)}$, (car $g^{(n)}-g^{(n+1)}=\rho_nJ^{(n)}w^{(n)}$)

On prend le produit scalaire par $w^{(n)}$ : $(0=)<g^{(n+1)},w^{(n)}>=<g^{(n)},w^{(n)}>-J_n<J^{(n)}w^{(n)},w^{(n)}>$
\[ \Ra\rho_n = \frac{ <-g^{(n)},w^{(n-1)}> }{ <J^{(n)}w^{(n)},w^{(n)}>} \]
\[ <J^{(n)}w^{(n)},w^{(n)}> >0 \textit{ car } \nabla f \textit{ est SDP}\]

4) Montrer que $|w^{(n)}| \leq (1+\frac{\alpha}{\beta})|g^{(n)}|$,
5) Montrer que $x_n$ converge.

\begin{exemple}
Soit $f:\R^2\ra\R$, $x\mapsto f(x)=x_1^2+x_2^2+4x_1x_2+2x_1-3x_2+7$, soit $x_0=(0,0)$.

Décrire la méthode du gradient à pas optimal pour calculer $\bar{x}$ tel que $\displaystyle f(\bar{x})=\inf_{x\in\R^2} f(x), \quad x=(x_1,x_2)$

On remarque que $f(x)=<Ax,x>-<b,x>+7$ où $A=\begin{pmatrix}1 & 2\\ 2 & 1\\ \end{pmatrix}$ et $b=\begin{pmatrix}-2\\ 3\\ \end{pmatrix}$

$q(x)=x_1^2+x_2^2+4x_1x_2 \Ra \phi(x, y)=x_1y_1+x_2y_2+2(x_1y_2+x_2y_1)=x_1(y_1+2y_2)+x_2(2y_1y2)=<x,Ay>$

$\nabla f(x)=2Ax-b$, $w^{(0)}=-\nabla f(x^{(0)})=-\nabla f(0)$, donc $w^{(0)}=b\ne 0 \Ra x^{(1)}=x^{(0)}+\rho_0w^{(0)}$ avec $\rho_0$ optimal dans la direction $w^{(0)}$.

On pose $\phi(t)=f(x^{(0)}+tw^{(0)})=f(tb)=<Atb,tb>-<b,tb>+7=t^2<Ab,b>-t<b,b>+7=\alpha t^2-\beta t+7$, $\phi'(t)=2\alpha t-\beta$

$\phi'(t)=0 \Lra t=\frac{\beta}{2\alpha}$ avec $\begin{cases} \beta=||b||^2 \\ \alpha=<Ab,b> \end{cases}$
$x^{(1)}=\frac{||b||^2}{2<Ab,b>}b$

$w^{(1)}=-\nabla f(x^{(1)})=-2Ax^{(1)}+b=-\frac{||b||^2}{<Ab,b>}Ab=+b= ...$

\end{exemple}



%\sectionEt{Conclusion}
%====================%


%Pour approfondir voir les références \cite{minoux} \cite{bierlaine} \cite{luenberger}


\bibliographystyle{./plain-fr}
\bibliography{biblio}


\end{document}
